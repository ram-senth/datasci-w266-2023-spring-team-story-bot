{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Ne5-am8spATz",
   "metadata": {
    "id": "Ne5-am8spATz"
   },
   "source": [
    "# Fine Tuning T5 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wHUaO6-oCKak",
   "metadata": {
    "id": "wHUaO6-oCKak"
   },
   "source": [
    "## One Time Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97nY_nL9CeiJ",
   "metadata": {
    "id": "97nY_nL9CeiJ"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "egOvt-Oq0M2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32608,
     "status": "ok",
     "timestamp": 1680807201563,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "egOvt-Oq0M2J",
    "outputId": "cb8bb004-f1d3-4ce4-ce7c-a3a2e8268b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/google-research/bleurt.git\n",
      "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-hyckydg0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-hyckydg0\n",
      "  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.4.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.10.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (0.1.97)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.32.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.1)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.53.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.20.3)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.7)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (4.5.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (67.6.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (16.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->BLEURT==0.0.2) (0.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.7.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.17.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (6.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.2.2)\n",
      "Building wheels for collected packages: BLEURT\n",
      "  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456781 sha256=daf5330a0a851ed353350a7ef5c0fb2d300aaef47d09b7e50ef981ad61ef9453\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hb0awoy6/wheels/b0/2a/c4/2bd63eb0e30d711baac17dfc89ca58a876cac68b44a2c2a97a\n",
      "Successfully built BLEURT\n",
      "Installing collected packages: BLEURT\n",
      "Successfully installed BLEURT-0.0.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.0.0+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.10.7)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19188 sha256=889552fab095788a90a2b2ed26e9be6789c2806f2f189e331e2b214408af4b52\n",
      "  Stored in directory: /root/.cache/pip/wheels/f6/d8/b0/15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3, accelerate\n",
      "Successfully installed accelerate-0.18.0 nvidia-ml-py3-7.352.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install git+https://github.com/google-research/bleurt.git\n",
    "!pip install setuptools accelerate nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtUED7nKSww0",
   "metadata": {
    "id": "RtUED7nKSww0"
   },
   "source": [
    "### Connect to Google Drive\n",
    "We will be loading data from google drive and also save trained models to google drive. So lets mount google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "DvSvLEFgSxWH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27385,
     "status": "ok",
     "timestamp": 1680807252082,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "DvSvLEFgSxWH",
    "outputId": "3de9e16a-c907-4e51-b59b-bf387da7a808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5QyOXIFTC1kO",
   "metadata": {
    "id": "5QyOXIFTC1kO"
   },
   "source": [
    "### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iyD4NJOTDDlt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8512,
     "status": "ok",
     "timestamp": 1680807264803,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iyD4NJOTDDlt",
    "outputId": "55c9d75a-7b04-4c94-9c4b-8b6f34d20fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.0.0+cu118\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "Utilization at the beginning:\n",
      "CPU RAM Used: 1.9 GB\n",
      "CPU RAM Free: 86.9 GB\n",
      "GPU memory occupied: 449 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Thu Apr  6 18:54:24 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0    46W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from pynvml import *\n",
    "import os,sys,humanize,psutil\n",
    "import gc\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "CHECKPOINTS_TO_SAVE = 1\n",
    "PROMPT = 'Generate next line: '\n",
    "SAVED_MODEL_PATH_FORMAT = 'drive/MyDrive/MIDS/w266/project/saved_models/final/{}-{}-finetuned'\n",
    "DATA_FILES_BASE_PATH = 'drive/MyDrive/MIDS/w266/project/datasci-w266-2023-spring-team-story-bot/data/'\n",
    "MAIN_DATA_FILE_FORMAT = 'posptproc_corpus_spacy_{}.csv'\n",
    "TRAIN_VAL_FILE_FORMAT = 'posptproc_corpus_spacy_{}_train_val.csv'\n",
    "TEST_FILE_FORMAT = 'posptproc_corpus_spacy_{}_test.csv'\n",
    "\n",
    "def print_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(\"CPU RAM Used: \" + humanize.naturalsize( psutil.virtual_memory().used))\n",
    "    print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available))\n",
    "\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Display details about the environment.\n",
    "print(f'torch.__version__: {torch.__version__}')\n",
    "!nvcc --version\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Utilization at the beginning:')\n",
    "print_utilization()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "JqnKi9o2_edx",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1680807264803,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "JqnKi9o2_edx"
   },
   "outputs": [],
   "source": [
    "# Helper Methods and classes\n",
    "# Create torch dataset\n",
    "class T5InputDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.inputs[\"input_ids\"][index].squeeze()\n",
    "        target_ids = self.targets[\"input_ids\"][index].squeeze()\n",
    "        attention_mask = self.inputs['attention_mask'][index].squeeze()\n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': target_ids}\n",
    "\n",
    "class TuningConfig:\n",
    "  def __init__(self, model_name, data_files_base_path, dataset, max_len, epochs, training_samples, val_samples, batch_size):\n",
    "    self.model_name = model_name\n",
    "    self.dataset = dataset\n",
    "    self.max_len = max_len\n",
    "    self.epochs = epochs\n",
    "    self.training_samples = training_samples\n",
    "    self.val_samples = val_samples\n",
    "    self.train_batch_size = batch_size\n",
    "    self.val_batch_size = 8\n",
    "    self.main_data_file = data_files_base_path + MAIN_DATA_FILE_FORMAT.format(dataset)\n",
    "    self.train_val_data_file = data_files_base_path + TRAIN_VAL_FILE_FORMAT.format(dataset)\n",
    "    self.test_data_file = data_files_base_path + TEST_FILE_FORMAT.format(dataset)\n",
    "    self.tuned_model_path = SAVED_MODEL_PATH_FORMAT.format(model_name, dataset)\n",
    "\n",
    "def load_data(main_file, train_val_file, test_file, test_seed=SEED, load_splits_from_file=False, prompt='', include_test=False, train_size=-1, val_size=-1):\n",
    "  def save_to(x, y, file_name):\n",
    "    xy = {'variable': x, 'label': y}\n",
    "    df = pd.DataFrame(xy)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "  def load_from(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df.astype({'variable':'string', 'label':'string'})\n",
    "    return df['variable'], df['label']\n",
    "\n",
    "  if load_splits_from_file:\n",
    "    x_train_val, y_train_val = load_from(train_val_file)\n",
    "    x_test, y_test = load_from(test_file)\n",
    "  else:\n",
    "    x, y = load_from(main_file)\n",
    "    # Split the dataset into train (80%), validation (10%) and test (10%) datasets.\n",
    "    # Test data should be determinable.\n",
    "    x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, train_size=0.9, random_state=test_seed)\n",
    "    # Save train-val and test data separately.\n",
    "    save_to(x_train_val, y_train_val, train_val_file)\n",
    "    save_to(x_test, y_test, test_file)\n",
    "\n",
    "  # Split train and validation datasets.\n",
    "  x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, train_size=0.88)\n",
    "\n",
    "  if train_size > 0:\n",
    "    x_train = x_train[:train_size]\n",
    "    y_train = y_train[:train_size]\n",
    "  if val_size > 0:\n",
    "    x_val = x_val[:val_size]\n",
    "    y_val = y_val[:val_size]\n",
    "\n",
    "  if prompt is not None and len(prompt) > 0:\n",
    "    x_train = prompt + x_train\n",
    "    x_val = prompt + x_val\n",
    "    x_test = prompt + x_test\n",
    "\n",
    "  if include_test:\n",
    "    return x_train, x_val, y_train, y_val, x_test, y_test\n",
    "  else:\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8J2hU012qkJn",
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1680811092407,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "8J2hU012qkJn"
   },
   "outputs": [],
   "source": [
    "TRAINING_SAMPLES = 100000\n",
    "TRAINING_SAMPLES = -1\n",
    "VAL_SAMPLES = 1000\n",
    "\n",
    "tuning_configs = [\n",
    "    TuningConfig('google/t5-v1_1-base', data_files_base_path=DATA_FILES_BASE_PATH, \n",
    "                 dataset='s1', max_len=65, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=128),\n",
    "    TuningConfig('google/t5-v1_1-base', data_files_base_path=DATA_FILES_BASE_PATH, \n",
    "                 dataset='s2', max_len=110, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=64),\n",
    "    TuningConfig('google/t5-v1_1-base', data_files_base_path=DATA_FILES_BASE_PATH,\n",
    "                 dataset='s3', max_len=150, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=64)\n",
    "]\n",
    "\n",
    "# DATA_NAME = \"s2\"\n",
    "# T5_MODEL_NAME = \"t5-small\"\n",
    "# T5_MODEL_NAME = \"t5-base\"\n",
    "# T5_MODEL_NAME = \"t5-large\" - colab instances do not have enough memory for T5 large.\n",
    "# T5_MODEL_NAME = 'google/t5-v1_1-small'\n",
    "# T5_MODEL_NAME = 'google/t5-v1_1-base'\n",
    "\n",
    "# MAIN_DATA_FILE = f'drive/MyDrive/MIDS/w266/project/datasci-w266-2023-spring-team-story-bot/posptproc_corpus_spacy_{DATA_NAME}.csv'\n",
    "# TRAIN_DATA_FILE = f'posptproc_corpus_spacy_{DATA_NAME}_train.csv'\n",
    "# VAL_DATA_FILE = f'posptproc_corpus_spacy_{DATA_NAME}_val.csv'\n",
    "\n",
    "# NUM_TRAIN_SAMPLES = 100000\n",
    "# # NUM_TRAIN_SAMPLES = 25000\n",
    "# # NUM_VAL_SAMPLES = 45000\n",
    "# NUM_VAL_SAMPLES = 1000\n",
    "# # MAX_LOAD_AT_ONCE = 10000\n",
    "# SRC_MAX_LENGTH=512\n",
    "# TARGET_MAX_LENGTH=128\n",
    "\n",
    "# MODEL_CKPT_FOLDER = 'drive/MyDrive/MIDS/w266/project/checkpoints/'\n",
    "# MODEL_CKPT_FILE = MODEL_CKPT_FOLDER + f'{T5_MODEL_NAME}-finetuned-02'\n",
    "# TUNED_T5_SAVED = f'drive/MyDrive/MIDS/w266/project/saved_models/final/{T5_MODEL_NAME}-data{DATA_NAME}-finetuned'\n",
    "# BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CzqQ3uHdCkdF",
   "metadata": {
    "id": "CzqQ3uHdCkdF"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "F94Z34ZEBLDz",
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1680807277336,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "F94Z34ZEBLDz"
   },
   "outputs": [],
   "source": [
    "def train(config, device):\n",
    "  def tokenize(tokenizer, data, max_length):\n",
    "    tokenized = tokenizer(\n",
    "      list(data),\n",
    "      max_length=max_length,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "  # Load the data\n",
    "  x_train, x_val, y_train, y_val = load_data(\n",
    "      config.main_data_file, config.train_val_data_file, \n",
    "      config.test_data_file, test_seed=SEED, \n",
    "      load_splits_from_file=True, prompt=PROMPT, include_test=False,\n",
    "      train_size=config.training_samples, val_size=config.val_samples)\n",
    "  print(f'x-train shape: {x_train.shape}, x-val shape: {x_val.shape}, y-train shape: {y_train.shape}, y-val shape: {y_val.shape}')\n",
    "\n",
    "  # Load the model\n",
    "  tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
    "  model = T5ForConditionalGeneration.from_pretrained(config.model_name).to(device)\n",
    "  print('Utilization after loading model:')\n",
    "  print_utilization()\n",
    "\n",
    "  # Tokenize data\n",
    "  x_train_tokenized = tokenize(tokenizer, x_train, config.max_len)\n",
    "  y_train_tokenized = tokenize(tokenizer, y_train, config.max_len)\n",
    "  x_val_tokenized = tokenize(tokenizer, x_val, config.max_len)\n",
    "  y_val_tokenized = tokenize(tokenizer, y_val, config.max_len)\n",
    "\n",
    "  training_set = T5InputDataset(x_train_tokenized, y_train_tokenized)\n",
    "  validation_set = T5InputDataset(x_val_tokenized, y_val_tokenized)\n",
    "\n",
    "  args = Seq2SeqTrainingArguments(\n",
    "      output_dir='checkpoints',\n",
    "      evaluation_strategy='epoch',\n",
    "      save_strategy='epoch',\n",
    "      per_device_train_batch_size=config.train_batch_size,\n",
    "      per_device_eval_batch_size=config.val_batch_size,\n",
    "      num_train_epochs=config.epochs,\n",
    "      load_best_model_at_end=True,\n",
    "      save_total_limit=CHECKPOINTS_TO_SAVE,\n",
    "      learning_rate=3e-4,\n",
    "      optim='adamw_torch',\n",
    "      # gradient_accumulation_steps=4,\n",
    "      # fp16=True,\n",
    "      bf16=True,\n",
    "      tf32=True\n",
    "  )\n",
    "\n",
    "  # Define the trainer, passing in the model, training args, and data generators\n",
    "  trainer = Seq2SeqTrainer(\n",
    "      model,\n",
    "      args,\n",
    "      train_dataset=training_set,\n",
    "      eval_dataset=validation_set\n",
    "  )\n",
    "\n",
    "  st = time.time()\n",
    "  result = trainer.train()\n",
    "  et = time.time()\n",
    "\n",
    "  # get the execution time\n",
    "  elapsed_time = et - st\n",
    "  print_summary(result)\n",
    "  print('Utilization after training: ')\n",
    "  print_utilization()  \n",
    "  \n",
    "  # Save the tuned model\n",
    "  trainer.save_model(config.tuned_model_path)\n",
    "\n",
    "  # Post training cleanup\n",
    "  trainer = None\n",
    "  model = None\n",
    "  with torch.no_grad():\n",
    "      torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  os.system('nvidia-smi -caa')\n",
    "  print('Utilization after post training cleanup: ')\n",
    "  print_utilization()  \n",
    "  print(f'{\"*\"*25}Training took {elapsed_time} seconds {\"*\"*25}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "B5ym-VcTs0Ty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c37428615e044024b7f073a75ce31797",
      "78055adf50d6491280350f197f4f5ba3",
      "4b51ea76c739483e9d1899c9081bad86",
      "1c5ea14d6b4d4df1aaa24c3b4ed6268b",
      "5361c854f91b4aaba1289651c8d81633",
      "ab7551d2398a4368bc236a9135d44bb8",
      "1b391d0cb2004ec689e053863edc71c0",
      "7cef2dbeffef4d68b8ff784c6bbf57c1",
      "971ca6bf76984a9ea1d3e6116ddda4d7",
      "fbadab6b2ce543dc92091967d11da5c1",
      "b098ea4c5e1945799f7540edb4e2aa20",
      "51844e16e7204bf2ba7ed4b941240388",
      "dae7614c550f4f5485fb87633290af33",
      "1c94f0729d9042d4a09d8a545c491126",
      "fe47d553391649b4b379377c86235c5d",
      "83d53558f0d74a648f2a25e0d965dbb5",
      "22eb74505acd448fbcc28b77184009e1",
      "8708be84995940dfb2eb7f9827486d77",
      "6f936551d94447539bd7d8c09357b75e",
      "9e654193c51848f7b1e9c4d323935825",
      "ab7c3cfe0aed48ee9dc14ba2de146fb5",
      "84013787bc75463596e792181686d328",
      "3452dedb7a984e91b1f3207f72a7d424",
      "a75d1526a28a4faeb87b34bf04200c4a",
      "954a4fa496bb4d0a82bc15cf0483f245",
      "110d6cdfb42843cdaeba924d7eddb596",
      "0a72a2b076644050882e42981c548289",
      "bfb74784d3c3498797051a648e33c564",
      "0e3a5fb42b314ab292477032ca5af0b0",
      "c358df596d114342a9a318d65d726e3a",
      "832246206ccb40a18c9b9e6a638e92f8",
      "82e2d6c426f54d2983aad25549224f5f",
      "bd009dd60f894ad3ba384863108e8a6a",
      "67fc078d083b44888c001f82b686a9ea",
      "9412486baa3d49d0a4d218cc72fddc7c",
      "5d5f80b2246247c2a99fb491bea1f836",
      "7bdab474fca74308a2e127359bebf74b",
      "d83db791c9314d84a05c26b62148c587",
      "f610602a39204bfeb4b292facab1e5ff",
      "e7c495f102c746b584ccbf47cc2c61b7",
      "b6b3676085fb4e05b8fbc83580a71d2a",
      "5cc7fd61f504472b86737cd553d36e38",
      "faf6dd62fa5a4f83aac78a70dbd0975d",
      "a8292a0597c24128ae27a8a7d76d3c13",
      "e1dec03ceeea40d3af5654d303155710",
      "e71bc40031e941aa974c973886e2bd18",
      "6afb784aac894725aff6905d757671bc",
      "9e4f2d1d289743ecb125f05b8b887e77",
      "6fe98187eda343388a512dc8fb958261",
      "c797f71222274707a71ebab91df5118a",
      "1c26f0dc567e4831be5661ad72478d08",
      "2d36765848084fedae7f7fd568a80363",
      "5d4c9c81f75f4fefad33e0e9abac7a88",
      "999babf371214279befda2c0b178e330",
      "fd098b9020584de5a63d3bfa7bc56792",
      "4f363329709a4ae9801b419a357d1001",
      "bddf118eee944ec7a1ef95160a74a41f",
      "01dcb36fcfe04f97b619013989ed2b01",
      "4003207d399d43f483e8b268dbbf0ece",
      "dfe50734b89a4601a2317d786ae1e47a",
      "04540af8f94c4b15ae718be4e0a286fa",
      "96f4efe93c684fb492e98da29b9a3e32",
      "026b74a1b9544debbbf9a98e3f6ab79a",
      "984fdf7e47f74dae91bb4bb96ceba157",
      "f056a957768a40459268734d9a5dd1bd",
      "ce015c8273894d82af9d1fac5f9f9e2c"
     ]
    },
    "executionInfo": {
     "elapsed": 1251738,
     "status": "ok",
     "timestamp": 1680808532100,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "B5ym-VcTs0Ty",
    "outputId": "15cde124-01b3-4ca5-9296-2d5606e6305b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training model google/t5-v1_1-base on s1 *************************\n",
      "x-train shape: (163302,), x-val shape: (1000,), y-train shape: (163302,), y-val shape: (1000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37428615e044024b7f073a75ce31797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51844e16e7204bf2ba7ed4b941240388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3452dedb7a984e91b1f3207f72a7d424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fc078d083b44888c001f82b686a9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dec03ceeea40d3af5654d303155710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f363329709a4ae9801b419a357d1001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilization after loading model:\n",
      "CPU RAM Used: 4.1 GB\n",
      "CPU RAM Free: 84.7 GB\n",
      "GPU memory occupied: 2443 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.9 GB\n",
      "Cached:    1.0 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3828' max='3828' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3828/3828 19:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.532500</td>\n",
       "      <td>1.357081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.416400</td>\n",
       "      <td>1.305090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.367600</td>\n",
       "      <td>1.274744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1159.56\n",
      "Samples/second: 422.49\n",
      "Utilization after training: \n",
      "CPU RAM Used: 4.8 GB\n",
      "CPU RAM Free: 83.9 GB\n",
      "GPU memory occupied: 29267 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 2.8 GB\n",
      "Cached:    26.8 GB\n",
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 4.8 GB\n",
      "CPU RAM Free: 83.9 GB\n",
      "GPU memory occupied: 1845 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "*************************Training took 1159.5720477104187 seconds *************************\n",
      "CPU times: user 18min 19s, sys: 2min 16s, total: 20min 36s\n",
      "Wall time: 20min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{\"*\"*25}Training model {tuning_configs[0].model_name} on {tuning_configs[0].dataset} {\"*\"*25}')\n",
    "train(tuning_configs[0], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "kmEumoJy2SVY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "executionInfo": {
     "elapsed": 2164305,
     "status": "ok",
     "timestamp": 1680810740008,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "kmEumoJy2SVY",
    "outputId": "17347926-6b91-457a-ad11-14e1b3255b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training model google/t5-v1_1-base on s2 *************************\n",
      "x-train shape: (162917,), x-val shape: (1000,), y-train shape: (162917,), y-val shape: (1000,)\n",
      "Utilization after loading model:\n",
      "CPU RAM Used: 5.9 GB\n",
      "CPU RAM Free: 82.8 GB\n",
      "GPU memory occupied: 2839 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.9 GB\n",
      "Cached:    1.0 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7638' max='7638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7638/7638 34:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.846100</td>\n",
       "      <td>0.730506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.808400</td>\n",
       "      <td>0.708829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.785100</td>\n",
       "      <td>0.701837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2065.85\n",
      "Samples/second: 236.59\n",
      "Utilization after training: \n",
      "CPU RAM Used: 6.1 GB\n",
      "CPU RAM Free: 82.7 GB\n",
      "GPU memory occupied: 26669 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 2.8 GB\n",
      "Cached:    24.3 GB\n",
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 6.0 GB\n",
      "CPU RAM Free: 82.7 GB\n",
      "GPU memory occupied: 1845 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "*************************Training took 2065.863163471222 seconds *************************\n",
      "CPU times: user 32min 57s, sys: 3min 9s, total: 36min 7s\n",
      "Wall time: 36min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{\"*\"*25}Training model {tuning_configs[1].model_name} on {tuning_configs[1].dataset} {\"*\"*25}')\n",
    "train(tuning_configs[1], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gwEgCdeS2Su6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "executionInfo": {
     "elapsed": 2880637,
     "status": "ok",
     "timestamp": 1680813986831,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "gwEgCdeS2Su6",
    "outputId": "f27fc06d-f855-4160-f2cb-ac58ea42923c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training model google/t5-v1_1-base on s3 *************************\n",
      "x-train shape: (162536,), x-val shape: (1000,), y-train shape: (162536,), y-val shape: (1000,)\n",
      "Utilization after loading model:\n",
      "CPU RAM Used: 7.6 GB\n",
      "CPU RAM Free: 81.1 GB\n",
      "GPU memory occupied: 8693 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 4.7 GB\n",
      "Cached:    6.7 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7620' max='7620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7620/7620 46:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>0.597351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.580931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.573198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2761.52\n",
      "Samples/second: 176.57\n",
      "Utilization after training: \n",
      "CPU RAM Used: 8.3 GB\n",
      "CPU RAM Free: 80.4 GB\n",
      "GPU memory occupied: 39595 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 6.5 GB\n",
      "Cached:    36.9 GB\n",
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 8.3 GB\n",
      "CPU RAM Free: 80.4 GB\n",
      "GPU memory occupied: 8693 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 3.8 GB\n",
      "Cached:    6.7 GB\n",
      "*************************Training took 2761.535127401352 seconds *************************\n",
      "CPU times: user 41min 20s, sys: 6min 49s, total: 48min 10s\n",
      "Wall time: 47min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{\"*\"*25}Training model {tuning_configs[2].model_name} on {tuning_configs[2].dataset} {\"*\"*25}')\n",
    "train(tuning_configs[2], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "p6D3Af1x5tL8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1680811068331,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "p6D3Af1x5tL8",
    "outputId": "3041ed96-a0e7-482d-d754-e8f2522e9fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 6.9 GB\n",
      "CPU RAM Free: 81.8 GB\n",
      "GPU memory occupied: 8693 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 3.8 GB\n",
      "Cached:    6.7 GB\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.system('nvidia-smi -caa')\n",
    "print('Utilization after post training cleanup: ')\n",
    "print_utilization()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ueuny-nWmRj",
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1680814573080,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "9ueuny-nWmRj"
   },
   "outputs": [],
   "source": [
    "# # Final test list for model trained against s2 dataset.\n",
    "# FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night. She could not sleep at all.',\n",
    "#                    'He stopped himself for a minute and thought if it was the right thing to do. It did seem like a good thing to do.',\n",
    "#                    'There once lived king named Rama. He was very wise and just.',\n",
    "#                    'Once upon a time, an old owl lived in the forest. He was very wise.']\n",
    "\n",
    "# Final test list for model trained against s1 dataset.\n",
    "FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night.',\n",
    "                   'He stopped himself for a minute and thought if it was the right thing to do.',\n",
    "                   'There once lived king named Rama.',\n",
    "                   'Once upon a time, an old owl lived in the forest.']\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, lines, prompt):\n",
    "  transformers.logging.set_verbosity_error()\n",
    "  for test_input_text in lines:\n",
    "      test_inputs = tokenizer([prompt + test_input_text], return_tensors='pt')\n",
    "      test_output_ids = model.generate(\n",
    "          test_inputs['input_ids'].cuda(),\n",
    "          num_beams=5,\n",
    "          no_repeat_ngram_size=3,\n",
    "          num_return_sequences=5,\n",
    "          max_new_tokens=100,\n",
    "          do_sample=True,\n",
    "          top_k=0)\n",
    "      print(f'Input: {test_input_text}')\n",
    "      decoded = [tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for out_ids in test_output_ids]\n",
    "      print(f'Output: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dOsRnXvJMY5C",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680814574901,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "dOsRnXvJMY5C"
   },
   "outputs": [],
   "source": [
    "## Untrained T5 model\n",
    "# evaluate(T5ForConditionalGeneration.from_pretrained(\"t5-large\").cuda(), t5_tokenizer, FINAL_TEST_LIST, \"Continue the next sentence of the story: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wxim_UyuNkF1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27034,
     "status": "ok",
     "timestamp": 1680814603260,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "wxim_UyuNkF1",
    "outputId": "4e093e18-06e6-4378-c6d2-cbf2bf3dbcfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating google/t5-v1_1-base tuned on s1 dataset\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['“I have a dream,” said she, “and I am going to tell you a story, and I will tell you all about it.”', 'Then she sat down on the sofa, and slept for a long time.', '\"It\\'s a great deal of work,\" she said.', 'Then he went to bed, and when she had slept a long time, he sat down on the bed.', '\"It is a long time since I heard the sound of a noise,\" she said, \"and I have been thinking of it for some time, and I don\\'t know what to do with it.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['Then he said, “It is a matter of business,” and he sat down to rest.', 'Then he said, “I am going to make a good deal of money.”', 'Then he said to himself: “It will be a pleasure to see you, and I will take care of you.”', 'Then he said, “I am going to tell you something, and I will show you what I have done, and what I can do for you, and how I can give you something to eat.”', 'Then he said, “It is my duty to take care of you.”']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['Then he came to a castle, where he lived with his wife and children.', '\"It\\'s a good thing,\" said he, \"but I don\\'t know what to do.', 'It was a king who had a daughter, and a son, who was to be married to a princess.', 'He was a king, and he had a daughter.', 'He had a son named Rama, who was a king, and he lived in a palace.']\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['Then the owl sat down on a branch of a tree, and said, “As long as I live, I shall never be able to get out of the forest.”', 'The owl, however, threw it into the water, and it sprang up from the ground, and swam away with it.', 'Then the owl said: \"It\\'s a good thing to have a little boy, and he\\'ll tell you all about it.\"', 'The old owl, however, did not know what to do.', \"Then the owl said, “It's time for me to go to bed.”\"]\n",
      "Evaluating google/t5-v1_1-base tuned on s2 dataset\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['She did not know what was to happen.', \"She slept soundly, and slept like a king's daughter.\", 'She sat down on the bed, and slept for a long time.', 'She was a very good girl, and she was very kind to her husband, who was very good to her, for she had a good heart, and was very fond of her.', 'She was a little frightened.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['\"I don\\'t see why I should do it,\" he said, \"but I think it\\'s the right thing to do.', '“I don’t think so,” he said.', 'Then he thought to himself, “It is a very good thing that I am going to do, and I am sure I shall be able to do it.”', 'Then he said to himself: “It is a good thing for you to know that I am not a thief.', 'He sat down and listened.']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['He had a wife and a daughter.', 'He had a wife, and a daughter, and they lived happily ever after.', 'He had a wife, and a daughter.', 'He had a wife, a daughter, and a son.', 'He had two sons, one of whom was a great king, and the other a king.']\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['The owl had a great appetite, and when he was hungry, he began to eat and eat, and sat down on a tree.', 'The owl lived in a tree, and sat on a branch of a birch tree.', 'The old owl was very fond of rabbits, and he was so fond of them that he would eat them if he could.', 'The owl had a long tail, which he wore on his back.', 'The owl was very fond of birds, and he was so fond of them that he did not know where to find them.']\n",
      "Evaluating google/t5-v1_1-base tuned on s3 dataset\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['\"It is a good thing, sir,\" said the prince.', '“I do not know,” said the old man, “but I am a little frightened.”', '“I am not a witch,” said the old woman, “but I am a king, and I have a wife, and a son, who is a princess, and has a daughter.”', '“I am afraid that you are not a very clever man,” said the prince, “but you are a clever man.', '\"I\\'ll tell you what it is,\" he said.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['“It’s a long time since I’ve been here,” said he.', '“What do you mean?” he asked.', '“I don’t know,” he said.', '“It is a matter of time,” said he, “but it is not yet time for me to go to bed.”', '“I am going to bury you,” he said, “and if I don’t see you, I’ll give you a piece of bread and a glass of wine.”']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['“What are you going to do with me?” said he, with a sigh.', 'Then he said, “Hast thou not seen thy father?”', '\"It\\'s a long time since I\\'ve been here, and I don\\'t know how long it\\'s been.', '“It’s a pity,” said the prince.', '“It’s a dreadful thing,” said he, “but I don’t want to be frightened.']\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['“It’s quite a different sort of place,” he said, “but it’s a very pleasant place.', '\"It\\'s a good thing,\" he said.', '\"Say,\" said the fox, \"I\\'ll tell you what\\'s the matter with the hare.\"', '\"Well, if you don\\'t like it,\" he said, \"you\\'ll have to go and see what\\'s the matter.', '“Then I will tell you what I want to know,” said he, “and I will show you the way to the Emerald City, and where I will find the Golden Fleece.”']\n"
     ]
    }
   ],
   "source": [
    "for config in tuning_configs:\n",
    "  tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
    "  print(f'Evaluating {config.model_name} tuned on {config.dataset} dataset')\n",
    "  evaluate(T5ForConditionalGeneration.from_pretrained(config.tuned_model_path).cuda(), tokenizer, FINAL_TEST_LIST, PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qy0KM1n3pHnE",
   "metadata": {
    "id": "qy0KM1n3pHnE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1qH9or9UvtJOb_mlSSHcRjZKwZT9uDwbl",
     "timestamp": 1680142704332
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
