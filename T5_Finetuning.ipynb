{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Ne5-am8spATz",
   "metadata": {
    "id": "Ne5-am8spATz"
   },
   "source": [
    "# Fine Tuning T5 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wHUaO6-oCKak",
   "metadata": {
    "id": "wHUaO6-oCKak"
   },
   "source": [
    "## One Time Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97nY_nL9CeiJ",
   "metadata": {
    "id": "97nY_nL9CeiJ"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egOvt-Oq0M2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29235,
     "status": "ok",
     "timestamp": 1680804727261,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "egOvt-Oq0M2J",
    "outputId": "29844232-0c1d-4922-b097-9d0bd4c7dc45"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install git+https://github.com/google-research/bleurt.git\n",
    "!pip install setuptools accelerate nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtUED7nKSww0",
   "metadata": {
    "id": "RtUED7nKSww0"
   },
   "source": [
    "### Connect to Google Drive\n",
    "We will be loading data from google drive and also save trained models to google drive. So lets mount google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DvSvLEFgSxWH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20869,
     "status": "ok",
     "timestamp": 1680804670370,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "DvSvLEFgSxWH",
    "outputId": "7ffc11ba-23c9-4bf4-990f-0afb964e72cb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5QyOXIFTC1kO",
   "metadata": {
    "id": "5QyOXIFTC1kO"
   },
   "source": [
    "### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iyD4NJOTDDlt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7961,
     "status": "ok",
     "timestamp": 1680804747405,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iyD4NJOTDDlt",
    "outputId": "3847be4e-a70d-4355-9b63-1d270de0719c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from pynvml import *\n",
    "import os,sys,humanize,psutil\n",
    "import gc\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "CHECKPOINTS_TO_SAVE = 1\n",
    "PROMPT = 'Generate next line: '\n",
    "SAVED_MODEL_PATH_FORMAT = 'drive/MyDrive/MIDS/w266/project/saved_models/final/{}-{}-finetuned'\n",
    "DATA_FILES_BASE_PATH = 'drive/MyDrive/MIDS/w266/project/datasci-w266-2023-spring-team-story-bot/data/'\n",
    "MAIN_DATA_FILE_FORMAT = 'posptproc_corpus_spacy_{}.csv'\n",
    "TRAIN_VAL_FILE_FORMAT = 'posptproc_corpus_spacy_{}_train_val.csv'\n",
    "TEST_FILE_FORMAT = 'posptproc_corpus_spacy_{}_test.csv'\n",
    "\n",
    "def print_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(\"CPU RAM Used: \" + humanize.naturalsize( psutil.virtual_memory().used))\n",
    "    print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available))\n",
    "\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Display details about the environment.\n",
    "print(f'torch.__version__: {torch.__version__}')\n",
    "!nvcc --version\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Utilization at the beginning:')\n",
    "print_utilization()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JqnKi9o2_edx",
   "metadata": {
    "id": "JqnKi9o2_edx"
   },
   "outputs": [],
   "source": [
    "# Helper Methods and classes\n",
    "# Create torch dataset\n",
    "class T5InputDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.inputs[\"input_ids\"][index].squeeze()\n",
    "        target_ids = self.targets[\"input_ids\"][index].squeeze()\n",
    "        attention_mask = self.inputs['attention_mask'][index].squeeze()\n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': target_ids}\n",
    "\n",
    "class TuningConfig:\n",
    "  def __init__(self, model_name, data_files_base_path, dataset, max_len, epochs, training_samples, val_samples, batch_size):\n",
    "    self.model_name = model_name\n",
    "    self.dataset = dataset\n",
    "    self.max_len = max_len\n",
    "    self.epochs = epochs\n",
    "    self.training_samples = training_samples\n",
    "    self.val_samples = val_samples\n",
    "    self.train_batch_size = batch_size\n",
    "    self.val_batch_size = 8\n",
    "    self.main_data_file = data_files_base_path + MAIN_DATA_FILE_FORMAT.format(dataset)\n",
    "    self.train_val_data_file = data_files_base_path + TRAIN_VAL_FILE_FORMAT.format(dataset)\n",
    "    self.test_data_file = data_files_base_path + TEST_FILE_FORMAT.format(dataset)\n",
    "    self.tuned_model_path = SAVED_MODEL_PATH_FORMAT.format(model_name, dataset)\n",
    "\n",
    "def load_data(main_file, train_val_file, test_file, test_seed=SEED, load_splits_from_file=False, prompt='', include_test=False, train_size=-1, val_size=-1):\n",
    "  def save_to(x, y, file_name):\n",
    "    xy = {'variable': x, 'label': y}\n",
    "    df = pd.DataFrame(xy)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "  def load_from(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df.astype({'variable':'string', 'label':'string'})\n",
    "    return df['variable'], df['label']\n",
    "\n",
    "  if load_splits_from_file:\n",
    "    x_train_val, y_train_val = load_from(train_val_file)\n",
    "    x_test, y_test = load_from(test_file)\n",
    "  else:\n",
    "    x, y = load_from(main_file)\n",
    "    # Split the dataset into train (80%), validation (10%) and test (10%) datasets.\n",
    "    # Test data should be determinable.\n",
    "    x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, train_size=0.9, random_state=test_seed)\n",
    "    # Save train-val and test data separately.\n",
    "    save_to(x_train_val, y_train_val, train_val_file)\n",
    "    save_to(x_test, y_test, test_file)\n",
    "\n",
    "  # Split train and validation datasets.\n",
    "  x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, train_size=0.88)\n",
    "\n",
    "  if train_size > 0:\n",
    "    x_train = x_train[:train_size]\n",
    "    y_train = y_train[:train_size]\n",
    "  if val_size > 0:\n",
    "    x_val = x_val[:val_size]\n",
    "    y_val = y_val[:val_size]\n",
    "\n",
    "  if prompt is not None and len(prompt) > 0:\n",
    "    x_train = prompt + x_train\n",
    "    x_val = prompt + x_val\n",
    "    x_test = prompt + x_test\n",
    "\n",
    "  if include_test:\n",
    "    return x_train, x_val, y_train, y_val, x_test, y_test\n",
    "  else:\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8J2hU012qkJn",
   "metadata": {
    "id": "8J2hU012qkJn"
   },
   "outputs": [],
   "source": [
    "TRAINING_SAMPLES = 100000\n",
    "VAL_SAMPLES = 1000\n",
    "\n",
    "tuning_configs = [\n",
    "    TuningConfig('google/t5-v1_1-base', data_files_base_path=DATA_FILES_BASE_PATH, \n",
    "                 dataset='s1', max_len=65, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=32),\n",
    "    TuningConfig('google/t5-v1_1-base', data_files_base_path=DATA_FILES_BASE_PATH, \n",
    "                 dataset='s2', max_len=110, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=32),\n",
    "    TuningConfig('google/t5-v1_1-base', data_files_base_path=DATA_FILES_BASE_PATH,\n",
    "                 dataset='s3', max_len=150, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=32)\n",
    "]\n",
    "\n",
    "# DATA_NAME = \"s2\"\n",
    "# T5_MODEL_NAME = \"t5-small\"\n",
    "# T5_MODEL_NAME = \"t5-base\"\n",
    "# T5_MODEL_NAME = \"t5-large\" - colab instances do not have enough memory for T5 large.\n",
    "# T5_MODEL_NAME = 'google/t5-v1_1-small'\n",
    "# T5_MODEL_NAME = 'google/t5-v1_1-base'\n",
    "\n",
    "# MAIN_DATA_FILE = f'drive/MyDrive/MIDS/w266/project/datasci-w266-2023-spring-team-story-bot/posptproc_corpus_spacy_{DATA_NAME}.csv'\n",
    "# TRAIN_DATA_FILE = f'posptproc_corpus_spacy_{DATA_NAME}_train.csv'\n",
    "# VAL_DATA_FILE = f'posptproc_corpus_spacy_{DATA_NAME}_val.csv'\n",
    "\n",
    "# NUM_TRAIN_SAMPLES = 100000\n",
    "# # NUM_TRAIN_SAMPLES = 25000\n",
    "# # NUM_VAL_SAMPLES = 45000\n",
    "# NUM_VAL_SAMPLES = 1000\n",
    "# # MAX_LOAD_AT_ONCE = 10000\n",
    "# SRC_MAX_LENGTH=512\n",
    "# TARGET_MAX_LENGTH=128\n",
    "\n",
    "# MODEL_CKPT_FOLDER = 'drive/MyDrive/MIDS/w266/project/checkpoints/'\n",
    "# MODEL_CKPT_FILE = MODEL_CKPT_FOLDER + f'{T5_MODEL_NAME}-finetuned-02'\n",
    "# TUNED_T5_SAVED = f'drive/MyDrive/MIDS/w266/project/saved_models/final/{T5_MODEL_NAME}-data{DATA_NAME}-finetuned'\n",
    "# BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CzqQ3uHdCkdF",
   "metadata": {
    "id": "CzqQ3uHdCkdF"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F94Z34ZEBLDz",
   "metadata": {
    "id": "F94Z34ZEBLDz"
   },
   "outputs": [],
   "source": [
    "def train(config, device):\n",
    "  def tokenize(tokenizer, data, max_length):\n",
    "    tokenized = tokenizer(\n",
    "      list(data),\n",
    "      max_length=max_length,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "  # Load the data\n",
    "  x_train, x_val, y_train, y_val = load_data(\n",
    "      config.main_data_file, config.train_val_data_file, \n",
    "      config.test_data_file, test_seed=SEED, \n",
    "      load_splits_from_file=True, prompt=PROMPT, include_test=False,\n",
    "      train_size=config.training_samples, val_size=config.val_samples)\n",
    "  print(f'x-train shape: {x_train.shape}, x-val shape: {x_val.shape}, y-train shape: {y_train.shape}, y-val shape: {y_val.shape}')\n",
    "\n",
    "  # Load the model\n",
    "  tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
    "  model = T5ForConditionalGeneration.from_pretrained(config.model_name).to(device)\n",
    "  print('Utilization after loading model:')\n",
    "  print_utilization()\n",
    "\n",
    "  # Tokenize data\n",
    "  x_train_tokenized = tokenize(tokenizer, x_train, config.max_len)\n",
    "  y_train_tokenized = tokenize(tokenizer, y_train, config.max_len)\n",
    "  x_val_tokenized = tokenize(tokenizer, x_val, config.max_len)\n",
    "  y_val_tokenized = tokenize(tokenizer, y_val, config.max_len)\n",
    "\n",
    "  training_set = T5InputDataset(x_train_tokenized, y_train_tokenized)\n",
    "  validation_set = T5InputDataset(x_val_tokenized, y_val_tokenized)\n",
    "\n",
    "  args = Seq2SeqTrainingArguments(\n",
    "      output_dir='checkpoints',\n",
    "      evaluation_strategy='epoch',\n",
    "      save_strategy='epoch',\n",
    "      per_device_train_batch_size=config.train_batch_size,\n",
    "      per_device_eval_batch_size=config.val_batch_size,\n",
    "      num_train_epochs=config.epochs,\n",
    "      load_best_model_at_end=True,\n",
    "      save_total_limit=CHECKPOINTS_TO_SAVE,\n",
    "      learning_rate=3e-4,\n",
    "      optim='adamw_torch',\n",
    "      # gradient_accumulation_steps=4,\n",
    "      # fp16=True,\n",
    "      bf16=True,\n",
    "      tf32=True\n",
    "  )\n",
    "\n",
    "  # Define the trainer, passing in the model, training args, and data generators\n",
    "  trainer = Seq2SeqTrainer(\n",
    "      model,\n",
    "      args,\n",
    "      train_dataset=training_set,\n",
    "      eval_dataset=validation_set\n",
    "  )\n",
    "\n",
    "  print(f'{\"*\"*25}Training model {config.model_name} on {config.dataset} {\"*\"*25}')\n",
    "  st = time.time()\n",
    "  result = trainer.train()\n",
    "  et = time.time()\n",
    "\n",
    "  # get the execution time\n",
    "  elapsed_time = et - st\n",
    "  print_summary(result)\n",
    "  print('Utilization after training: ')\n",
    "  print_utilization()  \n",
    "  \n",
    "  # Save the tuned model\n",
    "  trainer.save_model(config.tuned_model_path)\n",
    "\n",
    "  # Post training cleanup\n",
    "  trainer = None\n",
    "  model = None\n",
    "  with torch.no_grad():\n",
    "      torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  os.system('nvidia-smi -caa')\n",
    "  print('Utilization after post training cleanup: ')\n",
    "  print_utilization()  \n",
    "  print(f'{\"*\"*25}Training took {elapsed_time} seconds {\"*\"*25}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B5ym-VcTs0Ty",
   "metadata": {
    "id": "B5ym-VcTs0Ty"
   },
   "outputs": [],
   "source": [
    "# Train all model configurations.\n",
    "for config in tuning_configs:\n",
    "  train(config, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UBU7IM3uc14g",
   "metadata": {
    "id": "UBU7IM3uc14g"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fV2GYVEqIrQ8",
   "metadata": {
    "id": "fV2GYVEqIrQ8"
   },
   "outputs": [],
   "source": [
    "# # Final test list for model trained against s2 dataset.\n",
    "# FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night. She could not sleep at all.',\n",
    "#                    'He stopped himself for a minute and thought if it was the right thing to do. It did seem like a good thing to do.',\n",
    "#                    'There once lived king named Rama. He was very wise and just.',\n",
    "#                    'Once upon a time, an old owl lived in the forest. He was very wise.']\n",
    "\n",
    "# Final test list for model trained against s1 dataset.\n",
    "FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night.',\n",
    "                   'He stopped himself for a minute and thought if it was the right thing to do.',\n",
    "                   'There once lived king named Rama.',\n",
    "                   'Once upon a time, an old owl lived in the forest.']\n",
    "\n",
    "def evaluate(model, tokenizer, lines, prompt):\n",
    "  transformers.logging.set_verbosity_error()\n",
    "  for test_input_text in lines:\n",
    "      test_inputs = tokenizer([prompt + test_input_text], return_tensors='pt')\n",
    "      test_output_ids = model.generate(\n",
    "          test_inputs['input_ids'].cuda(),\n",
    "          num_beams=5,\n",
    "          no_repeat_ngram_size=3,\n",
    "          num_return_sequences=5,\n",
    "          max_new_tokens=100,\n",
    "          do_sample=True,\n",
    "          top_k=0)\n",
    "      print(f'Input: {test_input_text}')\n",
    "      decoded = [tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for out_ids in test_output_ids]\n",
    "      print(f'Output: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dOsRnXvJMY5C",
   "metadata": {
    "id": "dOsRnXvJMY5C"
   },
   "outputs": [],
   "source": [
    "## Untrained T5 model\n",
    "# evaluate(T5ForConditionalGeneration.from_pretrained(\"t5-large\").cuda(), t5_tokenizer, FINAL_TEST_LIST, \"Continue the next sentence of the story: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wxim_UyuNkF1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6109,
     "status": "ok",
     "timestamp": 1680764425972,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "wxim_UyuNkF1",
    "outputId": "01f0ca24-7768-44d6-f3e9-838745538f2a"
   },
   "outputs": [],
   "source": [
    "## Fine tuned T5 model\n",
    "evaluate(T5ForConditionalGeneration.from_pretrained(TUNED_T5_SAVED).cuda(), t5_tokenizer, FINAL_TEST_LIST, PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s631OVhjdXYP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1680218252131,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "s631OVhjdXYP",
    "outputId": "8792dfbf-5600-4167-fe5a-440549e9ca25"
   },
   "outputs": [],
   "source": [
    "TUNED_T5_SAVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LZWnzUGEz-GK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8674,
     "status": "ok",
     "timestamp": 1680299302218,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "LZWnzUGEz-GK",
    "outputId": "671a4a04-b653-4c4e-d374-8f4accb3021a"
   },
   "outputs": [],
   "source": [
    "evaluate(T5ForConditionalGeneration.from_pretrained(TUNED_T5_SAVED).cuda(), t5_tokenizer, FINAL_TEST_LIST, PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qy0KM1n3pHnE",
   "metadata": {
    "id": "qy0KM1n3pHnE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1qH9or9UvtJOb_mlSSHcRjZKwZT9uDwbl",
     "timestamp": 1680142704332
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
