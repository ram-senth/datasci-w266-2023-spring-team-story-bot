{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Ne5-am8spATz",
   "metadata": {
    "id": "Ne5-am8spATz"
   },
   "source": [
    "# Fine Tuning T5 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wHUaO6-oCKak",
   "metadata": {
    "id": "wHUaO6-oCKak"
   },
   "source": [
    "## One Time Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97nY_nL9CeiJ",
   "metadata": {
    "id": "97nY_nL9CeiJ"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "egOvt-Oq0M2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30974,
     "status": "ok",
     "timestamp": 1680756337837,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "egOvt-Oq0M2J",
    "outputId": "91056c64-de5b-492a-d1c7-4dd6336055e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.3 transformers-4.27.4\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/google-research/bleurt.git\n",
      "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-3aantiar\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-3aantiar\n",
      "  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.4.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.10.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (0.1.97)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2022.7.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.53.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (4.5.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (67.6.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.3.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.20.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.32.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.7)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (16.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.1)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->BLEURT==0.0.2) (0.0.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.7.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.17.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (6.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.0.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.2.2)\n",
      "Building wheels for collected packages: BLEURT\n",
      "  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456781 sha256=1e3490acc20f2e6f5d9b06b337b758cb39370c771aed829b981b920872be91a7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yozhzc0_/wheels/b0/2a/c4/2bd63eb0e30d711baac17dfc89ca58a876cac68b44a2c2a97a\n",
      "Successfully built BLEURT\n",
      "Installing collected packages: BLEURT\n",
      "Successfully installed BLEURT-0.0.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.0.0+cu118)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.10.7)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19188 sha256=e9a3adbf31e0534450b1160de165a7cc875f1eba4a7984517b5e0aa7f02a301e\n",
      "  Stored in directory: /root/.cache/pip/wheels/f6/d8/b0/15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3, accelerate\n",
      "Successfully installed accelerate-0.18.0 nvidia-ml-py3-7.352.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install git+https://github.com/google-research/bleurt.git\n",
    "!pip install setuptools accelerate nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "zQ3ahu389sGN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3547,
     "status": "ok",
     "timestamp": 1680756341382,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "zQ3ahu389sGN",
    "outputId": "cafae5df-7c09-41d1-b2bf-1bcbf4824c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.0.0+cu118\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'torch.__version__: {torch.__version__}')\n",
    "!nvcc --version\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b4vMiE-fEd",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1680727487704,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "82b4vMiE-fEd"
   },
   "outputs": [],
   "source": [
    "# # !git clone https://github.com/NVIDIA/apex\n",
    "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
    "# import torch\n",
    "# print(f'torch.__version__: {torch.__version__}')\n",
    "# torch.randn(1, 1, 32000).to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtUED7nKSww0",
   "metadata": {
    "id": "RtUED7nKSww0"
   },
   "source": [
    "### Connect to Google Drive\n",
    "We will be loading data from google drive and also save trained models to google drive. So lets mount google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "DvSvLEFgSxWH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22454,
     "status": "ok",
     "timestamp": 1680756363832,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "DvSvLEFgSxWH",
    "outputId": "54933ca4-8ae2-4155-8489-40790737504e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5QyOXIFTC1kO",
   "metadata": {
    "id": "5QyOXIFTC1kO"
   },
   "source": [
    "### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "iyD4NJOTDDlt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3868,
     "status": "ok",
     "timestamp": 1680756367697,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iyD4NJOTDDlt",
    "outputId": "f2579c61-3a77-4dbf-aae8-72a4444a9e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU RAM Used: 1.9 GB\n",
      "CPU RAM Free: 86.9 GB\n",
      "GPU memory occupied: 449 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Thu Apr  6 04:46:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   31C    P0    43W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from pynvml import *\n",
    "import os,sys,humanize,psutil\n",
    "import gc\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def print_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(\"CPU RAM Used: \" + humanize.naturalsize( psutil.virtual_memory().used))\n",
    "    print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available))\n",
    "\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_utilization()\n",
    "\n",
    "print_utilization()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8J2hU012qkJn",
   "metadata": {
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1680758314978,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "8J2hU012qkJn"
   },
   "outputs": [],
   "source": [
    "# HYPER_PARAMS = [{'model':'t5-base', 'dataset': 's1', 'max_len':65, 'epochs'=3, 'training_samples'=100000, 'val_samples'=1000, 'batch_size'=16},\n",
    "#                 {'model':'t5-base', 'dataset': 's2', 'max_len':110, 'epochs'=3, 'training_samples'=100000, 'val_samples'=1000, 'batch_size'=16},\n",
    "#                 {'model':'t5-base', 'dataset': 's3', 'max_len':150, 'epochs'=3, 'training_samples'=100000, 'val_samples'=1000, 'batch_size'=16}]\n",
    "\n",
    "DATA_NAME = \"s2\"\n",
    "# T5_MODEL_NAME = \"t5-small\"\n",
    "# T5_MODEL_NAME = \"t5-base\"\n",
    "# T5_MODEL_NAME = \"t5-large\" - colab instances do not have enough memory for T5 large.\n",
    "T5_MODEL_NAME = 'google/t5-v1_1-small'\n",
    "# T5_MODEL_NAME = 'google/t5-v1_1-base'\n",
    "\n",
    "MAIN_DATA_FILE = f'drive/MyDrive/MIDS/w266/project/datasci-w266-2023-spring-team-story-bot/posptproc_corpus_spacy_{DATA_NAME}.csv'\n",
    "TRAIN_DATA_FILE = f'posptproc_corpus_spacy_{DATA_NAME}_train.csv'\n",
    "VAL_DATA_FILE = f'posptproc_corpus_spacy_{DATA_NAME}_val.csv'\n",
    "SEED = 42\n",
    "CHECKPOINTS_TO_SAVE = 1\n",
    "\n",
    "NUM_TRAIN_SAMPLES = 100000\n",
    "# NUM_TRAIN_SAMPLES = 25000\n",
    "# NUM_VAL_SAMPLES = 45000\n",
    "NUM_VAL_SAMPLES = 1000\n",
    "# MAX_LOAD_AT_ONCE = 10000\n",
    "SRC_MAX_LENGTH=512\n",
    "TARGET_MAX_LENGTH=128\n",
    "\n",
    "# MODEL_CKPT_FOLDER = 'drive/MyDrive/MIDS/w266/project/checkpoints/'\n",
    "# MODEL_CKPT_FILE = MODEL_CKPT_FOLDER + f'{T5_MODEL_NAME}-finetuned-02'\n",
    "TUNED_T5_SAVED = f'drive/MyDrive/MIDS/w266/project/saved_models/final/{T5_MODEL_NAME}-data{DATA_NAME}-finetuned'\n",
    "PROMPT = 'Generate next line: '\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CzqQ3uHdCkdF",
   "metadata": {
    "id": "CzqQ3uHdCkdF"
   },
   "source": [
    "### Split Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "F94Z34ZEBLDz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3682,
     "status": "ok",
     "timestamp": 1680758325440,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "F94Z34ZEBLDz",
    "outputId": "740320f0-16fe-4491-b4db-db6c598be89a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 205705 entires to 143993 and 61712\n"
     ]
    }
   ],
   "source": [
    "def split_datafile(main_file, train_file, val_file):\n",
    "  data_df = pd.read_csv(main_file)\n",
    "  data_df = data_df.astype({'variable':'string', 'label':'string'})\n",
    "  \n",
    "  x_train, x_val, y_train, y_val = train_test_split(data_df['variable'], data_df['label'], train_size=0.7, random_state=SEED)\n",
    "  x_train = [PROMPT + x for x in x_train]\n",
    "  x_val = [PROMPT + x for x in x_val]\n",
    "  xy_train = {'variable': x_train, 'label': y_train}\n",
    "  xy_val = {'variable': x_val, 'label': y_val}\n",
    "\n",
    "  df_train = pd.DataFrame(xy_train)\n",
    "  df_val = pd.DataFrame(xy_val)\n",
    "  df_train.to_csv(train_file, index=False)\n",
    "  df_val.to_csv(val_file, index=False)\n",
    "  print(f'Split {data_df.shape[0]} entires to {df_train.shape[0]} and {df_val.shape[0]}')\n",
    "  return x_train[:NUM_TRAIN_SAMPLES], x_val[:NUM_VAL_SAMPLES], y_train[:NUM_TRAIN_SAMPLES], y_val[:NUM_VAL_SAMPLES]\n",
    "\n",
    "x_train, x_val, y_train, y_val = split_datafile(MAIN_DATA_FILE, TRAIN_DATA_FILE, VAL_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1LWac__z9Mx8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1680758327464,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "1LWac__z9Mx8",
    "outputId": "2a195998-50d7-4dc5-dc07-8f1316706008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 1000 100000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_val), len(y_train), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fw6I0qfQ0U5z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370,
     "referenced_widgets": [
      "079c6c032ef34836826b763a79bd37ce",
      "e8d73ed5d8c344a3bf23944d09dd3b02",
      "7fd718ad1210470ba5bb5157026572fc",
      "774635fed52c4b25ba25ae067657c565",
      "a5435d0fdaf14305ad339ac7b7ba5b0e",
      "94477645143145d087151e68d0a13cbf",
      "275d3234f5364ca283ca763053ed01f4",
      "b69646a8c33d41978335c356f6a38796",
      "02283bc3c9a7420d9875f00344771aa7",
      "6415c1acb2674956a60f34f9903a4383",
      "0269ceb188b4430cbcd75fab791ac832",
      "09316824f13f417e91244aafcb34ad8a",
      "d3ced7c5770a493dbfcda0e220f60cfd",
      "4440d784e1c94969bbb6585907080bda",
      "346a4d7f8ef34eaf9c1e747c08860540",
      "aef6aef93903496aa1eed2d2599a5af8",
      "a37d7c1bc5834fc78ba5048be15a391c",
      "48255ae259d24949a46a107292d03dcd",
      "a6e932faf88b46f494ea7517f2991cb1",
      "530848e626194eed8b35053fca114d93",
      "ace50b457b894ee88d498d9142527b20",
      "56fb4d771e2842fd9c03135651794411",
      "e8e32a21a777467d909176e61493482b",
      "3e17037d11fb4943896672ad123d9f59",
      "1b946298fe6a49258345f05192535266",
      "7afbf6ddb24c48cd99a2f35ead213d77",
      "c098cd924b71458b89f6b3d5ef438d12",
      "4fde594d5bde45d4b7de72ea5114a8a4",
      "5489b9c36b0b4249abef50960581dd4f",
      "4e6a7a793da34ce2a763205158e30fcb",
      "59402954e0934a2486bfd8f8c43e900f",
      "fdb11a819ed542d0b0a25649ce5aecab",
      "390f4ec3192548ad9dbb040de91429e5",
      "0101c3727d77489aa3d49c50d48290f7",
      "e3b3fa3ba9c745f3a0807bc7e996eb64",
      "744c287b85a7414584e2330d67781ada",
      "1e7eb224ddb3471591faa68083df0a6a",
      "aaf5c0136ced49cfb5aa8cbd9a447b41",
      "a5b0ff78120c4423a2e10a2a36d9b736",
      "d57fbd4f44a74f71b8a063db0b575992",
      "95af4451b2c941fb9391c057d3b79bbe",
      "b6e4c38cab7f4ce4b52ec1c5ef3a0dbd",
      "c5111f392aed4d059f810ca092133e42",
      "4806b34d62ee4272bca1f0d41b1ea20f",
      "186f724281ce4968b24ca9c44ae1738f",
      "50b26c49556c42a2a913eb728a8f853d",
      "51119d6be5ee4e79b435b8d58fa8900d",
      "5b696a716d0e41dcac672c7137a2ebd4",
      "a72c23071d50481484686b5d30b0bdc4",
      "7d0f1c77bb24470fbf0f46b364704491",
      "b9febfc4c76240ad9dfa0a071ae233bf",
      "25c3b7c982f7450ca92c74d14d37ddc6",
      "e531e275ae134b39b0e78a133d54533d",
      "97444c73899a491bb3c4088159dc68d8",
      "f260d23b9e804b0094a3342ad566774b",
      "6692e4e7cab54175ba2df973d5748b25",
      "d475075a4ea048d49593d74987257fb0",
      "bf1fdb1fce7949d7af5532996a891eed",
      "a796159b1f3a49a499e2529a0c86f583",
      "209ad09c81cd49d1838ec3fcf4f7fdb6",
      "465331e07b6841fca13d5b60d6ff0650",
      "dd390aa3783c4ea8b1de37a72d39e4ed",
      "8e26ba4e65ef42d298c88f8e384788d7",
      "425530f262bd44d0a415093cfb0298af",
      "0a07f8d60d634a898b4488d082f94eb5",
      "1427f0520db84ba389a8485a7535aec3"
     ]
    },
    "executionInfo": {
     "elapsed": 5384,
     "status": "ok",
     "timestamp": 1680758436991,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "fw6I0qfQ0U5z",
    "outputId": "3226f43c-3522-4202-b873-98945524f962"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079c6c032ef34836826b763a79bd37ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09316824f13f417e91244aafcb34ad8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e32a21a777467d909176e61493482b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0101c3727d77489aa3d49c50d48290f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186f724281ce4968b24ca9c44ae1738f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6692e4e7cab54175ba2df973d5748b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU RAM Used: 7.5 GB\n",
      "CPU RAM Free: 81.3 GB\n",
      "GPU memory occupied: 6709 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 3.7 GB\n",
      "Cached:    4.5 GB\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_NAME)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_MODEL_NAME).cuda()\n",
    "print_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "N1GVj5vCNLiN",
   "metadata": {
    "executionInfo": {
     "elapsed": 65843,
     "status": "ok",
     "timestamp": 1680758528175,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "N1GVj5vCNLiN"
   },
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, data, max_length, to_cuda=False):\n",
    "  tokenized = tokenizer(\n",
    "    list(data),\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt')\n",
    "  if to_cuda:\n",
    "    tokenized['input_ids'] = tokenized['input_ids'].cuda(non_blocking=True)\n",
    "    tokenized['attention_mask'] = tokenized['attention_mask'].cuda(non_blocking=True)\n",
    "  return tokenized\n",
    "  \n",
    "x_train_tokenized = tokenize(t5_tokenizer, x_train, SRC_MAX_LENGTH)\n",
    "y_train_tokenized = tokenize(t5_tokenizer, y_train, TARGET_MAX_LENGTH)\n",
    "x_val_tokenized = tokenize(t5_tokenizer, x_val, SRC_MAX_LENGTH)\n",
    "y_val_tokenized = tokenize(t5_tokenizer, y_val, TARGET_MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "r8-NFAkzBCD8",
   "metadata": {
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1680758578870,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "r8-NFAkzBCD8"
   },
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class ForT5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.inputs[\"input_ids\"][index].squeeze()\n",
    "        target_ids = self.targets[\"input_ids\"][index].squeeze()\n",
    "        attention_mask = self.inputs['attention_mask'][index].squeeze()\n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': target_ids}\n",
    "\n",
    "training_set = ForT5Dataset(x_train_tokenized, y_train_tokenized)\n",
    "validation_set = ForT5Dataset(x_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9DOI_NNdyHf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1680758598361,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "9DOI_NNdyHf1",
    "outputId": "c347ee0c-02da-4390-f064-545cbad99505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set), len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nf2HErDs0-XH",
   "metadata": {
    "executionInfo": {
     "elapsed": 643,
     "status": "ok",
     "timestamp": 1680758214388,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "nf2HErDs0-XH"
   },
   "outputs": [],
   "source": [
    "# def print_n(it, n=5):\n",
    "#   for i in range(n):\n",
    "#     print(f'{i+1}: {next(it)}')\n",
    "\n",
    "# print_n(train_data_iterator(), n=1)\n",
    "# print_n(val_data_iterator(), n=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pJZtrVdlXmlu",
   "metadata": {
    "id": "pJZtrVdlXmlu"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "_z2Zg6aUXlrJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "executionInfo": {
     "elapsed": 3179934,
     "status": "ok",
     "timestamp": 1680764379626,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "_z2Zg6aUXlrJ",
    "outputId": "0db55478-2e6c-4a94-b4bf-122b0f54b7b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1211: FutureWarning: `--adafactor` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--optim adafactor` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 52:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.761200</td>\n",
       "      <td>0.709733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.727400</td>\n",
       "      <td>0.688406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>0.679588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3179.66\n",
      "Samples/second: 94.35\n",
      "CPU RAM Used: 8.2 GB\n",
      "CPU RAM Free: 80.5 GB\n",
      "GPU memory occupied: 15245 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.9 GB\n",
      "Cached:    12.9 GB\n",
      "CPU times: user 53min 5s, sys: 12.3 s, total: 53min 17s\n",
      "Wall time: 52min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=CHECKPOINTS_TO_SAVE,\n",
    "    # learning_rate=3e-4,\n",
    "    # optim='adamw_torch',\n",
    "    learning_rate=1e-3,\n",
    "    adafactor=True,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    tf32=True\n",
    ")\n",
    "\n",
    "# Define the trainer, passing in the model, training args, and data generators\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    t5_model,\n",
    "    args,\n",
    "    train_dataset=training_set,\n",
    "    eval_dataset=validation_set\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7SXzYHTn2Y18",
   "metadata": {
    "executionInfo": {
     "elapsed": 8247,
     "status": "ok",
     "timestamp": 1680764396627,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "7SXzYHTn2Y18"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(TUNED_T5_SAVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f-h_b2R8N63X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1680764405653,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "f-h_b2R8N63X",
    "outputId": "179b4c75-5105-49c8-bba1-510abf7e9b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Accounted PIDs for GPU 00000000:00:04.0.\n",
      "All done.\n",
      "CPU RAM Used: 8.2 GB\n",
      "CPU RAM Free: 80.5 GB\n",
      "GPU memory occupied: 4529 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.9 GB\n",
      "Cached:    2.4 GB\n"
     ]
    }
   ],
   "source": [
    "# Post training cleanup\n",
    "trainer = None\n",
    "t5_model = None\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "!nvidia-smi -caa\n",
    "print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UBU7IM3uc14g",
   "metadata": {
    "id": "UBU7IM3uc14g"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fV2GYVEqIrQ8",
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1680764411997,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "fV2GYVEqIrQ8"
   },
   "outputs": [],
   "source": [
    "# # Final test list for model trained against s2 dataset.\n",
    "# FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night. She could not sleep at all.',\n",
    "#                    'He stopped himself for a minute and thought if it was the right thing to do. It did seem like a good thing to do.',\n",
    "#                    'There once lived king named Rama. He was very wise and just.',\n",
    "#                    'Once upon a time, an old owl lived in the forest. He was very wise.']\n",
    "\n",
    "# Final test list for model trained against s1 dataset.\n",
    "FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night.',\n",
    "                   'He stopped himself for a minute and thought if it was the right thing to do.',\n",
    "                   'There once lived king named Rama.',\n",
    "                   'Once upon a time, an old owl lived in the forest.']\n",
    "\n",
    "def evaluate(model, tokenizer, lines, prompt):\n",
    "  transformers.logging.set_verbosity_error()\n",
    "  for test_input_text in lines:\n",
    "      test_inputs = tokenizer([prompt + test_input_text], return_tensors='pt')\n",
    "      test_output_ids = model.generate(\n",
    "          test_inputs['input_ids'].cuda(),\n",
    "          num_beams=5,\n",
    "          no_repeat_ngram_size=3,\n",
    "          num_return_sequences=5,\n",
    "          max_new_tokens=100,\n",
    "          do_sample=True,\n",
    "          top_k=0)\n",
    "      print(f'Input: {test_input_text}')\n",
    "      decoded = [tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for out_ids in test_output_ids]\n",
    "      print(f'Output: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dOsRnXvJMY5C",
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1680764417093,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "dOsRnXvJMY5C"
   },
   "outputs": [],
   "source": [
    "## Untrained T5 model\n",
    "# evaluate(T5ForConditionalGeneration.from_pretrained(\"t5-large\").cuda(), t5_tokenizer, FINAL_TEST_LIST, \"Continue the next sentence of the story: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "wxim_UyuNkF1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6109,
     "status": "ok",
     "timestamp": 1680764425972,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "wxim_UyuNkF1",
    "outputId": "01f0ca24-7768-44d6-f3e9-838745538f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['\"It\\'s a pity,\" she said.', 'She sat on the bed, and she slept for a long time.', 'She sat down on the bed, and said, “Alas!', 'She was a beautiful princess, and she was as beautiful as the sun, and the sun was shining brightly on the sky.', 'She sat on the bed.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['He had a frightened face, and he had sat down on the floor and listened to the sound of a thunderbolt.', '“It’s the right thing,” he said.', '“Oh, I’m going to give you a piece of money,” he said.', '\"It\\'s the best thing you can do,\" he said.', 'He did not know what he was thinking about.']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['He was a king, and he had a great deal of money in his pocket.', \"He was the king's son, and he was a king.\", 'He was a king, and he had a son.', 'He was a rich man, and he was rich.', 'He was a king, and he lived in a cave in the forest.']\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['He was a owl, and if he had been a little boy, he would have been frightened to death.', 'The owl was a very beautiful fellow, and he had a great acorns on his back.', 'He had a long, long tail, and he was a young owl.', \"He was a very handsome man, and had a long tail, and a lion's tail, which was as big as a man’s head, and his tail was so big that he could not move.\", 'He was very fond of owls, and he loved them so much.']\n"
     ]
    }
   ],
   "source": [
    "## Fine tuned T5 model\n",
    "evaluate(T5ForConditionalGeneration.from_pretrained(TUNED_T5_SAVED).cuda(), t5_tokenizer, FINAL_TEST_LIST, PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s631OVhjdXYP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1680218252131,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "s631OVhjdXYP",
    "outputId": "8792dfbf-5600-4167-fe5a-440549e9ca25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'drive/MyDrive/MIDS/w266/project/saved_models/t5-base-datas1-finetuned'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TUNED_T5_SAVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LZWnzUGEz-GK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8674,
     "status": "ok",
     "timestamp": 1680299302218,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "LZWnzUGEz-GK",
    "outputId": "671a4a04-b653-4c4e-d374-8f4accb3021a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Princess Leia lay upon her bed all the night. She could not sleep at all.\n",
      "Output: ['She slept in a slumber, but she did not know how to get out of bed.', 'She was very ill, and she could not sleep for a long time.', 'She was so tired that she could not sleep at all.', 'She could not sleep at all.', 'She could not sleep at all.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do. It did seem like a good thing to do.\n",
      "Output: ['He went out to eat a little, and then he went to bed, and he sat down with a cup of tea, and said, “It’s a good thing to do.”', 'It was a good thing to do.', 'It was a good thing to do.', 'It was a good thing to do.', 'He thought it was the right thing to do.']\n",
      "Input: There once lived king named Rama. He was very wise and just.\n",
      "Output: ['Rama was a king of India, and he had a great wealth of wealth.', 'Rama was a very good king, and he was very good to his people.', 'Rama was a good king, and he had a great wealth of wealth.', 'Rama was a very good king.', 'He had a great wealth of wealth and good fortune.']\n",
      "Input: Once upon a time, an old owl lived in the forest. He was very wise.\n",
      "Output: ['He was very wise, and he had a great deal of wisdom.', 'He was very clever.', \"He knew how to use a pen, and if he had any money he would be able to give him a piece of hay to eat, and when he was done, he went to the owl's house, where he sat and waited for a long time.\", 'He said that if he was wise, he would not be wiser than a owl.', 'He was a great owl, and he was very wise.']\n"
     ]
    }
   ],
   "source": [
    "evaluate(T5ForConditionalGeneration.from_pretrained(TUNED_T5_SAVED).cuda(), t5_tokenizer, FINAL_TEST_LIST, PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qy0KM1n3pHnE",
   "metadata": {
    "id": "qy0KM1n3pHnE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1qH9or9UvtJOb_mlSSHcRjZKwZT9uDwbl",
     "timestamp": 1680142704332
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
