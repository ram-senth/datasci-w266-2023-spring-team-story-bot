{"cells":[{"cell_type":"markdown","source":["# One Time Setup"],"metadata":{"id":"wHUaO6-oCKak"},"id":"wHUaO6-oCKak"},{"cell_type":"markdown","source":["## Install Dependencies"],"metadata":{"id":"97nY_nL9CeiJ"},"id":"97nY_nL9CeiJ"},{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install git+https://github.com/google-research/bleurt.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egOvt-Oq0M2J","executionInfo":{"status":"ok","timestamp":1680128558267,"user_tz":420,"elapsed":24253,"user":{"displayName":"Ram S","userId":"17279396566363655986"}},"outputId":"3ea099cb-620d-4e05-d590-2b2daf9b75fe"},"id":"egOvt-Oq0M2J","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/google-research/bleurt.git\n","  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-5ai2ktxg\n","  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-5ai2ktxg\n","  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.4.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.10.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (2.11.0)\n","Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.1.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (0.1.97)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2022.7.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.31.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.2.0)\n","Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.11.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.3.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.51.3)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (4.5.0)\n","Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.11.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.19.6)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.15.0)\n","Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.11.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (67.6.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (16.0.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.40.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (2.16.3)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (2.2.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (2.27.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (3.4.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (0.6.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (6.1.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (2022.12.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->BLEURT==0.0.2) (3.2.2)\n","Building wheels for collected packages: BLEURT\n","  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456781 sha256=e8b0ba029c1302d4b7522cd64c0925d195174667cd369649e1224659d72b8398\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-obsr6kbw/wheels/b0/2a/c4/2bd63eb0e30d711baac17dfc89ca58a876cac68b44a2c2a97a\n","Successfully built BLEURT\n","Installing collected packages: BLEURT\n","Successfully installed BLEURT-0.0.2\n"]}]},{"cell_type":"markdown","source":["## Connect to Google Drive\n","In order to avoid disk space issues, we will use google drive for saving model snapshots during training."],"metadata":{"id":"RtUED7nKSww0"},"id":"RtUED7nKSww0"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvSvLEFgSxWH","executionInfo":{"status":"ok","timestamp":1680128675295,"user_tz":420,"elapsed":86290,"user":{"displayName":"Ram S","userId":"17279396566363655986"}},"outputId":"65404ce4-bb1e-4b65-dcd0-8560692e5338"},"id":"DvSvLEFgSxWH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Imports and Constants"],"metadata":{"id":"5QyOXIFTC1kO"},"id":"5QyOXIFTC1kO"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.model_selection import train_test_split\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import torch\n","import transformers\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","MAIN_DATA_FILE = 'posptproc_corpus_spacy_s1.csv'\n","TRAIN_DATA_FILE = 'posptproc_corpus_spacy_s1_train.csv'\n","VAL_DATA_FILE = 'posptproc_corpus_spacy_s1_val.csv'\n","NUM_TRAIN_SAMPLES = 110000 # Start with a small sample size.\n","NUM_VAL_SAMPLES = 45000 # Start with a small sample size.\n","MAX_LOAD_AT_ONCE = 2500\n","MAX_TOKEN_LENGTH = 128\n","\n","MODEL_CKPT_FOLDER = 'drive/MyDrive/MIDS/w266/project/checkpoints/'\n","MODEL_CKPT_FILE = MODEL_CKPT_FOLDER + 't5base-finetuned'\n","TUNED_T5_SAVED = 'drive/MyDrive/MIDS/w266/project/saved_models/t5base_finetuned'\n","PROMPT = 'generate next line: '\n","BATCH_SIZE = 16\n","SEED = 42"],"metadata":{"id":"iyD4NJOTDDlt","executionInfo":{"status":"ok","timestamp":1680134976294,"user_tz":420,"elapsed":2,"user":{"displayName":"Ram S","userId":"17279396566363655986"}}},"id":"iyD4NJOTDDlt","execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Split Data File"],"metadata":{"id":"CzqQ3uHdCkdF"},"id":"CzqQ3uHdCkdF"},{"cell_type":"code","source":["def split_datafile(main_file, train_file, val_file):\n","  data_df = pd.read_csv(main_file)\n","  data_wc = data_df[(data_df['variable'].str.split(' ').str.len() > 3) & \n","     (data_df['variable'].str.split(' ').str.len() < 50) &\n","     (data_df['label'].str.split(' ').str.len() > 3) &\n","     (data_df['label'].str.split(' ').str.len() < 50)]\n","  x_train, x_val, y_train, y_val = train_test_split(data_wc['variable'], data_wc['label'], train_size=0.7)\n","  xy_train = {'variable': [PROMPT + x for x in x_train], 'label': y_train}\n","  xy_val = {'variable': [PROMPT + x for x in x_val], 'label': y_val}\n","\n","  df_train = pd.DataFrame(xy_train)\n","  df_val = pd.DataFrame(xy_val)\n","  df_train.to_csv(train_file, index=False)\n","  df_val.to_csv(val_file, index=False)\n","  print(f'Split {data_df.shape[0]} entires to {df_train.shape[0]} and {df_val.shape[0]}')\n","\n","split_datafile(MAIN_DATA_FILE, TRAIN_DATA_FILE, VAL_DATA_FILE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F94Z34ZEBLDz","executionInfo":{"status":"ok","timestamp":1680128784073,"user_tz":420,"elapsed":5931,"user":{"displayName":"Ram S","userId":"17279396566363655986"}},"outputId":"ad62cc22-534a-47ea-ccfa-324bbbb433cb"},"id":"F94Z34ZEBLDz","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Split 206190 entires to 114486 and 49066\n"]}]},{"cell_type":"code","source":["class StoryDataIterator:    \n","    def __init__(self,\n","                 tokenizer,\n","                 n_examples,\n","                 max_load_at_once,\n","                 data_filename,\n","                 max_length=128,\n","                 shuffle=True):\n","        \n","        self.tokenizer = tokenizer\n","        self.n_examples = n_examples\n","        self.max_load_at_once = max_load_at_once\n","        self.data_filename = data_filename\n","        self.max_length = max_length\n","        self.shuffle = shuffle\n","        \n","        # Initialize row order, call on_epoch_end to shuffle row indices\n","        self.row_order = np.arange(1, self.n_examples+1)\n","        self.on_epoch_end()\n","\n","        # Load first chunk of max_load_at_once examples\n","        self.df_curr_loaded = self._load_next_chunk(0)\n","        self.curr_idx_in_load = 0\n","\n","    def preprocess_data(self, text_pair, max_length=128):\n","        orig_text, target_text = text_pair\n","        orig_encoded = self.tokenizer.batch_encode_plus(\n","            [orig_text],\n","            max_length=max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        orig_input_ids = orig_encoded['input_ids'][0]\n","        orig_attention_mask = orig_encoded['attention_mask'][0]\n","        \n","        target_encoded = self.tokenizer.batch_encode_plus(\n","            [target_text],\n","            max_length=max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        \n","        label_ids = target_encoded['input_ids'][0]\n","        \n","        return {'input_ids': orig_input_ids,\n","                'attention_mask': orig_attention_mask,\n","                'labels': label_ids}\n","\n","    def _load_next_chunk(self, idx):\n","        load_start = idx\n","        load_end = idx + self.max_load_at_once\n","\n","        # Indices to skip are the ones in the shuffled row_order before and\n","        # after the chunk we'll use for this chunk\n","        load_idx_skip = self.row_order[:load_start] + self.row_order[load_end:]\n","        self.df_curr_loaded = pd.read_csv(self.data_filename, skiprows=load_idx_skip)\n","        self.df_curr_loaded = self.df_curr_loaded.sample(frac=1)\n","    \n","    def __len__(self):\n","        return self.n_examples\n","    \n","    def __getitem__(self, idx):\n","        if self.df_curr_loaded is None or self.curr_idx_in_load >= len(self.df_curr_loaded):\n","            self._load_next_chunk(idx)\n","            self.curr_idx_in_load = 0\n","        \n","        text_pair = self.df_curr_loaded[['variable', 'label']].values.astype(str)[self.curr_idx_in_load]\n","        self.curr_idx_in_load += 1\n","        \n","        item_data = self.preprocess_data(\n","            text_pair,\n","            self.max_length\n","        )\n","        \n","        return item_data\n","    \n","    def __call__(self):\n","        for i in range(self.__len__()):\n","            yield self.__getitem__(i)\n","            \n","            if i == self.__len__()-1:\n","                self.on_epoch_end()\n","    \n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            self.row_order = list(np.random.permutation(self.row_order))"],"metadata":{"id":"OF-DmVGV_rT4"},"id":"OF-DmVGV_rT4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n","t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n","\n","# MAX_SRC_LEN = 512\n","# MAX_TARGET_LEN = 128\n","\n","\n","# input_ids = t5_tokenizer(\"continue the story: The house is wonderful.\", return_tensors=\"pt\").input_ids\n","# labels = t5_tokenizer(\"It is small but warm and welcoming.\", return_tensors=\"pt\").input_ids\n","\n","# # the forward function automatically creates the correct decoder_input_ids\n","# loss = t5_model(input_ids=input_ids, labels=labels).loss\n","# loss.item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fw6I0qfQ0U5z","executionInfo":{"status":"ok","timestamp":1680129095040,"user_tz":420,"elapsed":2109,"user":{"displayName":"Ram S","userId":"17279396566363655986"}},"outputId":"5f3f2252-fa09-47f5-f316-1ae7270da85b"},"id":"fw6I0qfQ0U5z","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["train_data_iterator = StoryDataIterator(\n","    tokenizer=t5_tokenizer,\n","    n_examples=NUM_TRAIN_SAMPLES,\n","    max_load_at_once=MAX_LOAD_AT_ONCE,\n","    data_filename=TRAIN_DATA_FILE,\n","    max_length=MAX_TOKEN_LENGTH\n",")\n","\n","val_data_iterator = StoryDataIterator(\n","    tokenizer=t5_tokenizer,\n","    n_examples=NUM_VAL_SAMPLES,\n","    max_load_at_once=MAX_LOAD_AT_ONCE,\n","    data_filename=VAL_DATA_FILE,\n","    max_length=MAX_TOKEN_LENGTH\n",")"],"metadata":{"id":"r8-NFAkzBCD8"},"id":"r8-NFAkzBCD8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def print_n(it, n=5):\n","#   for i in range(n):\n","#     print(f'{i+1}: {next(it)}')\n","\n","# print_n(train_data_iterator(), n=1)\n","# print_n(val_data_iterator(), n=1)\n"],"metadata":{"id":"nf2HErDs0-XH"},"id":"nf2HErDs0-XH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train Model"],"metadata":{"id":"pJZtrVdlXmlu"},"id":"pJZtrVdlXmlu"},{"cell_type":"code","source":["args = Seq2SeqTrainingArguments(\n","    MODEL_CKPT_FILE,\n","    evaluation_strategy='epoch',\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    num_train_epochs=3,\n",")\n","\n","# Define the trainer, passing in the model, training args, and data generators\n","\n","trainer = Seq2SeqTrainer(\n","    t5_model,\n","    args,\n","    train_dataset=train_data_iterator,\n","    eval_dataset=val_data_iterator\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"_z2Zg6aUXlrJ","outputId":"b5dbb44b-9963-44d1-e18f-7f748ad52ac8","executionInfo":{"status":"ok","timestamp":1680134517678,"user_tz":420,"elapsed":1421836,"user":{"displayName":"Ram S","userId":"17279396566363655986"}}},"id":"_z2Zg6aUXlrJ","execution_count":20,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15603' max='20625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15603/20625 1:05:28 < 21:04, 3.97 it/s, Epoch 2.27/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.788400</td>\n","      <td>0.769117</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.759400</td>\n","      <td>0.762675</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='20625' max='20625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [20625/20625 1:29:10, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.788400</td>\n","      <td>0.769117</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.759400</td>\n","      <td>0.762675</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.751900</td>\n","      <td>0.762736</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=20625, training_loss=0.7788078920306581, metrics={'train_runtime': 5351.1225, 'train_samples_per_second': 61.669, 'train_steps_per_second': 3.854, 'total_flos': 1.116569862144e+16, 'train_loss': 0.7788078920306581, 'epoch': 3.0})"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["trainer.save_model(TUNED_T5_SAVED)"],"metadata":{"id":"7SXzYHTn2Y18","executionInfo":{"status":"ok","timestamp":1680135052728,"user_tz":420,"elapsed":1554,"user":{"displayName":"Ram S","userId":"17279396566363655986"}}},"id":"7SXzYHTn2Y18","execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"UBU7IM3uc14g"},"id":"UBU7IM3uc14g"},{"cell_type":"code","source":["## Inference\n","t5_model_loaded = T5ForConditionalGeneration.from_pretrained(TUNED_T5_SAVED).cuda()\n","transformers.logging.set_verbosity_error()\n","for test_input_text in ['The princess lay upon her bed all the night.',\n","                        'He stopped himself for a minute and thought if it was the right thing to do.',\n","                        'There once lived king named Rama.',\n","                        'Once upon a time, an old owl lived in the forest.']:\n","    test_inputs = t5_tokenizer([PROMPT + test_input_text], return_tensors='pt')\n","    test_output_ids = t5_model_loaded.generate(test_inputs['input_ids'].cuda())\n","\n","    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n","                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxim_UyuNkF1","executionInfo":{"status":"ok","timestamp":1680135169586,"user_tz":420,"elapsed":1904,"user":{"displayName":"Ram S","userId":"17279396566363655986"}},"outputId":"bc216eb7-23ba-48bc-b9e2-c01a5c8e11ff"},"id":"wxim_UyuNkF1","execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["['She was a very happy woman, and she was very happy to be a princess.']\n","['“It is not the right thing,” he said.']\n","['Rama was a king, and he was a king.']\n","['Then he sat down and sat down, and he s']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"s631OVhjdXYP"},"id":"s631OVhjdXYP","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["97nY_nL9CeiJ"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":5}