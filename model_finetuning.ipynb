{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Ne5-am8spATz",
   "metadata": {
    "id": "Ne5-am8spATz"
   },
   "source": [
    "# Fine Tuning T5 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wHUaO6-oCKak",
   "metadata": {
    "id": "wHUaO6-oCKak"
   },
   "source": [
    "## One Time Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97nY_nL9CeiJ",
   "metadata": {
    "id": "97nY_nL9CeiJ"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "egOvt-Oq0M2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19759,
     "status": "ok",
     "timestamp": 1680841018274,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "egOvt-Oq0M2J",
    "outputId": "29d2c17f-061c-4e4f-e975-0163814f26e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/google-research/bleurt.git\n",
      "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-4_pimu89\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-4_pimu89\n",
      "  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.4.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.10.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from BLEURT==0.0.2) (0.1.97)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->BLEURT==0.0.2) (2022.7.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (16.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (4.5.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.3.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (23.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (67.6.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.20.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.32.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (1.53.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.7)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->BLEURT==0.0.2) (0.0.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.17.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.2.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.27.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (6.1.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.2.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.18.0)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.9/dist-packages (7.352.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.0.0+cu118)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.10.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install git+https://github.com/google-research/bleurt.git\n",
    "!pip install setuptools accelerate nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtUED7nKSww0",
   "metadata": {
    "id": "RtUED7nKSww0"
   },
   "source": [
    "### Connect to Google Drive\n",
    "We will be loading data from google drive and also save trained models to google drive. So lets mount google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "DvSvLEFgSxWH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24092,
     "status": "ok",
     "timestamp": 1680840752712,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "DvSvLEFgSxWH",
    "outputId": "da54406e-f9cb-4ee2-db1e-956f856ae1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5QyOXIFTC1kO",
   "metadata": {
    "id": "5QyOXIFTC1kO"
   },
   "source": [
    "### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "iyD4NJOTDDlt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1680842514192,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iyD4NJOTDDlt",
    "outputId": "650337ec-c163-4978-a534-9c792ddf1f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.0.0+cu118\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "Utilization at the beginning:\n",
      "CPU RAM Used: 6.6 GB\n",
      "CPU RAM Free: 82.1 GB\n",
      "GPU memory occupied: 6835 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    4.9 GB\n",
      "Fri Apr  7 04:41:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    49W / 400W |   6389MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, Trainer, TrainingArguments\n",
    "from pynvml import *\n",
    "import os,sys,humanize,psutil\n",
    "import gc\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "CHECKPOINTS_TO_SAVE = 1\n",
    "T5_PROMPT = 'Generate next line: '\n",
    "OPT_PROMPT = ''\n",
    "SAVED_MODEL_PATH_FORMAT = 'drive/MyDrive/MIDS/w266/project/saved_models/final/{}-{}-finetuned'\n",
    "DATA_FILES_BASE_PATH = 'drive/MyDrive/MIDS/w266/project/datasci-w266-2023-spring-team-story-bot/data/'\n",
    "MAIN_DATA_FILE_FORMAT = 'posptproc_corpus_spacy_{}.csv'\n",
    "TRAIN_VAL_FILE_FORMAT = 'posptproc_corpus_spacy_{}_train_val.csv'\n",
    "TEST_FILE_FORMAT = 'posptproc_corpus_spacy_{}_test.csv'\n",
    "\n",
    "def print_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(\"CPU RAM Used: \" + humanize.naturalsize( psutil.virtual_memory().used))\n",
    "    print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available))\n",
    "\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Display details about the environment.\n",
    "print(f'torch.__version__: {torch.__version__}')\n",
    "!nvcc --version\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Utilization at the beginning:')\n",
    "print_utilization()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "JqnKi9o2_edx",
   "metadata": {
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1680842519492,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "JqnKi9o2_edx"
   },
   "outputs": [],
   "source": [
    "# Helper Methods and classes\n",
    "# Create torch dataset\n",
    "class T5InputDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.inputs[\"input_ids\"][index].squeeze()\n",
    "        target_ids = self.targets[\"input_ids\"][index].squeeze()\n",
    "        attention_mask = self.inputs['attention_mask'][index].squeeze()\n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': target_ids}\n",
    "\n",
    "class OptInputDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.labels = labels\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids']) \n",
    "\n",
    "class TuningConfig:\n",
    "  def __init__(self, model_name, dataset, max_len, epochs, \n",
    "               training_samples, val_samples, batch_size, \n",
    "               trainer_provider, datasets_provider, prompt):\n",
    "    self.model_name = model_name\n",
    "    self.dataset = dataset\n",
    "    self.max_len = max_len\n",
    "    self.epochs = epochs\n",
    "    self.training_samples = training_samples\n",
    "    self.val_samples = val_samples\n",
    "    self.train_batch_size = batch_size\n",
    "    self.val_batch_size = 8\n",
    "    self.main_data_file = DATA_FILES_BASE_PATH + MAIN_DATA_FILE_FORMAT.format(dataset)\n",
    "    self.train_val_data_file = DATA_FILES_BASE_PATH + TRAIN_VAL_FILE_FORMAT.format(dataset)\n",
    "    self.test_data_file = DATA_FILES_BASE_PATH + TEST_FILE_FORMAT.format(dataset)\n",
    "    self.tuned_model_path = SAVED_MODEL_PATH_FORMAT.format(model_name, dataset)\n",
    "    self.trainer_provider = trainer_provider\n",
    "    self.datasets_provider = datasets_provider\n",
    "    self.prompt = prompt\n",
    "\n",
    "def load_data(main_file, train_val_file, test_file, test_seed=SEED, load_splits_from_file=False, prompt='', include_test=False, train_size=-1, val_size=-1):\n",
    "  def save_to(x, y, file_name):\n",
    "    xy = {'variable': x, 'label': y}\n",
    "    df = pd.DataFrame(xy)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "  def load_from(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df.astype({'variable':'string', 'label':'string'})\n",
    "    return df['variable'], df['label']\n",
    "\n",
    "  if load_splits_from_file:\n",
    "    x_train_val, y_train_val = load_from(train_val_file)\n",
    "    x_test, y_test = load_from(test_file)\n",
    "  else:\n",
    "    x, y = load_from(main_file)\n",
    "    # Split the dataset into train (80%), validation (10%) and test (10%) datasets.\n",
    "    # Test data should be determinable.\n",
    "    x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, train_size=0.9, random_state=test_seed)\n",
    "    # Save train-val and test data separately.\n",
    "    save_to(x_train_val, y_train_val, train_val_file)\n",
    "    save_to(x_test, y_test, test_file)\n",
    "\n",
    "  # Split train and validation datasets.\n",
    "  x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, train_size=0.88)\n",
    "\n",
    "  if train_size > 0:\n",
    "    x_train = x_train[:train_size]\n",
    "    y_train = y_train[:train_size]\n",
    "  if val_size > 0:\n",
    "    x_val = x_val[:val_size]\n",
    "    y_val = y_val[:val_size]\n",
    "\n",
    "  if prompt is not None and len(prompt) > 0:\n",
    "    x_train = prompt + x_train\n",
    "    x_val = prompt + x_val\n",
    "    x_test = prompt + x_test\n",
    "\n",
    "  if include_test:\n",
    "    return x_train, x_val, y_train, y_val, x_test, y_test\n",
    "  else:\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "def t5_datasets_provider(config, x_train, y_train, x_val, y_val):\n",
    "  def tokenize(tokenizer, data, max_length):\n",
    "    tokenized = tokenizer(\n",
    "      list(data),\n",
    "      max_length=max_length,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')\n",
    "    return tokenized\n",
    "  # Tokenize data\n",
    "  tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "  x_train_tokenized = tokenize(tokenizer, x_train, config.max_len)\n",
    "  y_train_tokenized = tokenize(tokenizer, y_train, config.max_len)\n",
    "  x_val_tokenized = tokenize(tokenizer, x_val, config.max_len)\n",
    "  y_val_tokenized = tokenize(tokenizer, y_val, config.max_len)\n",
    "  # Create and return datasets\n",
    "  training_set = T5InputDataset(x_train_tokenized, y_train_tokenized)\n",
    "  validation_set = T5InputDataset(x_val_tokenized, y_val_tokenized)\n",
    "  return training_set, validation_set\n",
    "\n",
    "def opt_datasets_provider(config, x_train, y_train, x_val, y_val):\n",
    "  def tokenize(tokenizer, data, max_length):\n",
    "    return tokenizer(\n",
    "        list(data),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt')\n",
    "  tokenizer = AutoTokenizer.from_pretrained(config.model_name)    \n",
    "  x_train_tokenized = tokenize(tokenizer, x_train, config.max_len)\n",
    "  x_val_tokenized = tokenize(tokenizer, x_val, config.max_len)\n",
    "  training_set = OptInputDataset(x_train_tokenized, x_train_tokenized['input_ids'])#, y_train_tk['input_ids'])\n",
    "  validation_set = OptInputDataset(x_val_tokenized, x_val_tokenized['input_ids'])#, y_test_tk['input_ids'])\n",
    "  return training_set, validation_set\n",
    "\n",
    "def t5_trainer_provider(config, training_set, validation_set, device):\n",
    "  # Create trainer\n",
    "  model = T5ForConditionalGeneration.from_pretrained(config.model_name).to(device)\n",
    "  print(f'Utilization after loading model {config.model_name}:')\n",
    "  print_utilization()\n",
    "\n",
    "  args = Seq2SeqTrainingArguments(\n",
    "      output_dir='checkpoints',\n",
    "      evaluation_strategy='epoch',\n",
    "      save_strategy='epoch',\n",
    "      per_device_train_batch_size=config.train_batch_size,\n",
    "      per_device_eval_batch_size=config.val_batch_size,\n",
    "      num_train_epochs=config.epochs,\n",
    "      load_best_model_at_end=True,\n",
    "      save_total_limit=CHECKPOINTS_TO_SAVE,\n",
    "      optim='adamw_torch',\n",
    "      learning_rate=3e-4,\n",
    "      # gradient_accumulation_steps=4,\n",
    "      # fp16=True,\n",
    "      bf16=True,\n",
    "      tf32=True\n",
    "  )\n",
    "\n",
    "  # Define the trainer, passing in the model, training args, and data generators\n",
    "  trainer = Seq2SeqTrainer(\n",
    "      model,\n",
    "      args,\n",
    "      train_dataset=training_set,\n",
    "      eval_dataset=validation_set\n",
    "  )\n",
    "  return trainer\n",
    "\n",
    "def opt_trainer_provider(config, training_set, validation_set, device):\n",
    "  model = OPTForCausalLM.from_pretrained(config.model_name).to(device)\n",
    "  training_args = TrainingArguments(\n",
    "      output_dir='checkpoints', \n",
    "      evaluation_strategy=\"epoch\",\n",
    "      save_strategy=\"epoch\", \n",
    "      per_device_train_batch_size=config.train_batch_size,\n",
    "      per_device_eval_batch_size=config.val_batch_size,\n",
    "      num_train_epochs=config.epochs,\n",
    "      load_best_model_at_end=True,\n",
    "      save_total_limit=CHECKPOINTS_TO_SAVE,\n",
    "      optim='adamw_torch',\n",
    "      # learning_rate=3e-4,\n",
    "      # # gradient_accumulation_steps=4,\n",
    "      # # fp16=True,\n",
    "      # bf16=True,\n",
    "      # tf32=True\n",
    "    )\n",
    "  trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_set,\n",
    "    eval_dataset=validation_set,\n",
    "    compute_metrics=None,\n",
    "  )\n",
    "  return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8J2hU012qkJn",
   "metadata": {
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1680842549394,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "8J2hU012qkJn"
   },
   "outputs": [],
   "source": [
    "TRAINING_SAMPLES = 100000\n",
    "TRAINING_SAMPLES = -1\n",
    "VAL_SAMPLES = 1000\n",
    "\n",
    "tuning_configs = [\n",
    "    TuningConfig('google/t5-v1_1-base', \n",
    "                 dataset='s1', max_len=65, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=128,\n",
    "                 trainer_provider=t5_trainer_provider,\n",
    "                 datasets_provider=t5_datasets_provider,\n",
    "                 prompt=T5_PROMPT),\n",
    "    TuningConfig('google/t5-v1_1-base', \n",
    "                 dataset='s2', max_len=110, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=64,\n",
    "                 trainer_provider=t5_trainer_provider,\n",
    "                 datasets_provider=t5_datasets_provider,\n",
    "                 prompt=T5_PROMPT),\n",
    "    TuningConfig('google/t5-v1_1-base', \n",
    "                 dataset='s3', max_len=150, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=64,\n",
    "                 trainer_provider=t5_trainer_provider,\n",
    "                 datasets_provider=t5_datasets_provider,\n",
    "                 prompt=T5_PROMPT),\n",
    "    TuningConfig('facebook/opt-350m', \n",
    "                 dataset='s2', max_len=110, epochs=3, training_samples=TRAINING_SAMPLES,\n",
    "                 val_samples=VAL_SAMPLES, batch_size=64,\n",
    "                 trainer_provider=opt_trainer_provider,\n",
    "                 datasets_provider=opt_datasets_provider,\n",
    "                 prompt=OPT_PROMPT)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CzqQ3uHdCkdF",
   "metadata": {
    "id": "CzqQ3uHdCkdF"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "F94Z34ZEBLDz",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680842554633,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "F94Z34ZEBLDz"
   },
   "outputs": [],
   "source": [
    "def train(config, device):\n",
    "  # Load the data\n",
    "  x_train, x_val, y_train, y_val = load_data(\n",
    "      config.main_data_file, config.train_val_data_file, \n",
    "      config.test_data_file, test_seed=SEED, \n",
    "      load_splits_from_file=True, prompt=config.prompt, include_test=False,\n",
    "      train_size=config.training_samples, val_size=config.val_samples)\n",
    "  print(f'x-train shape: {x_train.shape}, x-val shape: {x_val.shape}, y-train shape: {y_train.shape}, y-val shape: {y_val.shape}')\n",
    "\n",
    "  # Get the dataset objects.\n",
    "  training_set, validation_set = config.datasets_provider(config, x_train, y_train, x_val, y_val)\n",
    "  \n",
    "  # Get trainer.\n",
    "  trainer = config.trainer_provider(config, training_set, validation_set, device)\n",
    "\n",
    "  # Train the model.\n",
    "  st = time.time()\n",
    "  result = trainer.train()\n",
    "  et = time.time()\n",
    "\n",
    "  # Print training summary\n",
    "  elapsed_time = et - st\n",
    "  print_summary(result)\n",
    "  print('Utilization after training: ')\n",
    "  print_utilization()  \n",
    "  \n",
    "  # Save the tuned model\n",
    "  trainer.save_model(config.tuned_model_path)\n",
    "\n",
    "  # Post training cleanup\n",
    "  trainer = None\n",
    "  with torch.no_grad():\n",
    "      torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  os.system('nvidia-smi -caa')\n",
    "  print('Utilization after post training cleanup: ')\n",
    "  print_utilization()  \n",
    "  print(f'{\"*\"*25}Training took {elapsed_time} seconds {\"*\"*25}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "B5ym-VcTs0Ty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "eaf6becab9814df3a73654c7a80c281b",
      "9c01a1cfa9994109ba9a6780d8e42eef",
      "3462fb926c9a4a038012e75617e87a6f",
      "a6364a85bd304df4be82c6717ef183fd",
      "54a44893ea2040088066fd43dec46633",
      "2e1c090a56a04dcf9139632f50ef76f5",
      "0229a726629a418984f1c7deb83f17fb",
      "ee733206e4f44451a3db867de4f60965",
      "30bab2edd32a4a5cb5afc8ee804397ab",
      "9338276bdbf44b3eb8df148c69cf891a",
      "c740334ded384e7884e6fc6c6515ccc4",
      "b7e63a80188c4d7c8e0142f999e954d6",
      "782d6c211d0342c8b39a2c8861788e81",
      "b9840e33c3974e61885e82b3327788a8",
      "01b8f05e058f46a8b1d290857b478371",
      "591612c41b3d4df5ab590a638d28da74",
      "c3bb86c179d641e1a27c35040efcf2e2",
      "55b5d15d64bb4576aecf4f9865a15a4f",
      "bbe75ea9fc7f4cb086e0fd1b12cb87b9",
      "21334e9404a9448dba7711f770d6a4b7",
      "d6ffea375fa241d49d7fc52b10e735ee",
      "9d8c2e5fce344c9b92b5d30ff3c01261",
      "5681e1b5914b4a76a6550d9eaa17af9b",
      "25138d3b22d84a1caadfe9639803c7c2",
      "93e391fb98724716a82d84a7407b3f54",
      "6bda944db9b943e4b26407c0192eaf85",
      "b2ad3da8ff1a453a95f876b1841d1aa3",
      "9de2cecdbbe34e878490d3704d4cef29",
      "71371f7deab841988ef0f05a61d608d5",
      "329f8e4d6be747498fca93270b218aa1",
      "1869d91f36dc46e78bdb861951d70384",
      "b87628a6c2c343ed8cc8f687e25d72ad",
      "e08609dacf0f4149991acb0b36b16bb4",
      "36b1e4be252f4a34b06b98edf7db2c1d",
      "99b5622b402b4a0eac2f16897343618e",
      "13028b10aeda4388bad24bd4f6329ede",
      "4dc5db5aa24d4a3b92cea64a21b9e15d",
      "c5798052bae6403f9cb3c48719baea43",
      "a27660443a7a432398a1f9a12d3198e6",
      "852586413d764797b7cba7592238ab4c",
      "28e24e42e37045c8ac403c3118d03e06",
      "6bc307a7e0e248a788cf60a36b94fd57",
      "eec25560d7e5463eb9826113bff095a8",
      "73414bffcb9b4242b5c337e2f1b714a7",
      "2d696dbd7a754a5cafa70d331c84e015",
      "483de31a826f453abef4320f4605dda2",
      "85f81ca0e3bb4d93afa05594ef679933",
      "c62a74d0cf8c4d69853ec573eddf8751",
      "892e359470f44449a3021cc708e2106c",
      "3280f84cc8ee4b62ad2c0d2df4b88b90",
      "3d485bbace084fec924fe4ad6bf76cea",
      "40a46223d62c4af4a1054e53f6b72e5b",
      "a7ce329e6cf64d309dfeea118e777f38",
      "4aa63f14c5ec4cf0a9c4fa60d4a4635f",
      "cfd663e6dd1e4e07940c6dcf54d1ee4e",
      "f9d5b89d14ce40809f7438b791231d06",
      "c1855449647141018bcb526491be3b3c",
      "0233f0692a4e49f88756658bec42c970",
      "a639e6e05848460580a945b12e038d0f",
      "75e94ed7de364883ad020d378b17c9c6",
      "49864e871e644139836eafac1bcd6e7f",
      "b7687ca6ea6840749898dc2160adc832",
      "11526e132f80462196da95bf7013f93d",
      "1ea69fcefce74e3cacea6b133b5a9fc1",
      "7b9e749f7a35478a8fe5125e15863ca2",
      "a9af7060fa66415bafc51f4eea9ec514"
     ]
    },
    "executionInfo": {
     "elapsed": 60919,
     "status": "ok",
     "timestamp": 1680841107361,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "B5ym-VcTs0Ty",
    "outputId": "8dd70ad6-6d6e-4918-8f6a-77990945d536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training model google/t5-v1_1-base on s1 *************************\n",
      "x-train shape: (50,), x-val shape: (50,), y-train shape: (50,), y-val shape: (50,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf6becab9814df3a73654c7a80c281b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e63a80188c4d7c8e0142f999e954d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5681e1b5914b4a76a6550d9eaa17af9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b1e4be252f4a34b06b98edf7db2c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d696dbd7a754a5cafa70d331c84e015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d5b89d14ce40809f7438b791231d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilization after loading model google/t5-v1_1-base:\n",
      "CPU RAM Used: 4.0 GB\n",
      "CPU RAM Free: 84.8 GB\n",
      "GPU memory occupied: 2443 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.9 GB\n",
      "Cached:    1.0 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>38.133545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>35.575703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>33.866104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26.02\n",
      "Samples/second: 5.76\n",
      "Utilization after training: \n",
      "CPU RAM Used: 4.2 GB\n",
      "CPU RAM Free: 84.5 GB\n",
      "GPU memory occupied: 14555 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 2.8 GB\n",
      "Cached:    12.5 GB\n",
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 4.2 GB\n",
      "CPU RAM Free: 84.5 GB\n",
      "GPU memory occupied: 1839 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "*************************Training took 26.03862690925598 seconds *************************\n",
      "CPU times: user 16.2 s, sys: 14 s, total: 30.3 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{\"*\"*25}Training model {tuning_configs[0].model_name} on {tuning_configs[0].dataset} {\"*\"*25}')\n",
    "train(tuning_configs[0], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmEumoJy2SVY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "executionInfo": {
     "elapsed": 2164305,
     "status": "ok",
     "timestamp": 1680810740008,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "kmEumoJy2SVY",
    "outputId": "17347926-6b91-457a-ad11-14e1b3255b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training model google/t5-v1_1-base on s2 *************************\n",
      "x-train shape: (162917,), x-val shape: (1000,), y-train shape: (162917,), y-val shape: (1000,)\n",
      "Utilization after loading model:\n",
      "CPU RAM Used: 5.9 GB\n",
      "CPU RAM Free: 82.8 GB\n",
      "GPU memory occupied: 2839 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.9 GB\n",
      "Cached:    1.0 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7638' max='7638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7638/7638 34:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.846100</td>\n",
       "      <td>0.730506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.808400</td>\n",
       "      <td>0.708829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.785100</td>\n",
       "      <td>0.701837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2065.85\n",
      "Samples/second: 236.59\n",
      "Utilization after training: \n",
      "CPU RAM Used: 6.1 GB\n",
      "CPU RAM Free: 82.7 GB\n",
      "GPU memory occupied: 26669 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 2.8 GB\n",
      "Cached:    24.3 GB\n",
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 6.0 GB\n",
      "CPU RAM Free: 82.7 GB\n",
      "GPU memory occupied: 1845 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "*************************Training took 2065.863163471222 seconds *************************\n",
      "CPU times: user 32min 57s, sys: 3min 9s, total: 36min 7s\n",
      "Wall time: 36min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{\"*\"*25}Training model {tuning_configs[1].model_name} on {tuning_configs[1].dataset} {\"*\"*25}')\n",
    "train(tuning_configs[1], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gwEgCdeS2Su6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "executionInfo": {
     "elapsed": 2880637,
     "status": "ok",
     "timestamp": 1680813986831,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "gwEgCdeS2Su6",
    "outputId": "f27fc06d-f855-4160-f2cb-ac58ea42923c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training model google/t5-v1_1-base on s3 *************************\n",
      "x-train shape: (162536,), x-val shape: (1000,), y-train shape: (162536,), y-val shape: (1000,)\n",
      "Utilization after loading model:\n",
      "CPU RAM Used: 7.6 GB\n",
      "CPU RAM Free: 81.1 GB\n",
      "GPU memory occupied: 8693 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 4.7 GB\n",
      "Cached:    6.7 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7620' max='7620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7620/7620 46:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>0.597351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.580931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.573198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2761.52\n",
      "Samples/second: 176.57\n",
      "Utilization after training: \n",
      "CPU RAM Used: 8.3 GB\n",
      "CPU RAM Free: 80.4 GB\n",
      "GPU memory occupied: 39595 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 6.5 GB\n",
      "Cached:    36.9 GB\n",
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 8.3 GB\n",
      "CPU RAM Free: 80.4 GB\n",
      "GPU memory occupied: 8693 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 3.8 GB\n",
      "Cached:    6.7 GB\n",
      "*************************Training took 2761.535127401352 seconds *************************\n",
      "CPU times: user 41min 20s, sys: 6min 49s, total: 48min 10s\n",
      "Wall time: 47min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{\"*\"*25}Training model {tuning_configs[2].model_name} on {tuning_configs[2].dataset} {\"*\"*25}')\n",
    "train(tuning_configs[2], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "QKqUoDag8AVw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "executionInfo": {
     "elapsed": 2183021,
     "status": "ok",
     "timestamp": 1680844756394,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "QKqUoDag8AVw",
    "outputId": "010a849c-4eb3-4782-a1bc-b059eab684aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training model facebook/opt-350m on s2 *************************\n",
      "x-train shape: (162917,), x-val shape: (1000,), y-train shape: (162917,), y-val shape: (1000,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7638' max='7638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7638/7638 36:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.394800</td>\n",
       "      <td>1.352503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.232100</td>\n",
       "      <td>1.275371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.119300</td>\n",
       "      <td>1.245987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2160.93\n",
      "Samples/second: 226.18\n",
      "Utilization after training: \n",
      "CPU RAM Used: 9.1 GB\n",
      "CPU RAM Free: 79.7 GB\n",
      "GPU memory occupied: 26329 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 3.7 GB\n",
      "Cached:    24.0 GB\n",
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 8.7 GB\n",
      "CPU RAM Free: 80.0 GB\n",
      "GPU memory occupied: 1839 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "*************************Training took 2160.9419729709625 seconds *************************\n",
      "CPU times: user 34min 41s, sys: 5min 58s, total: 40min 39s\n",
      "Wall time: 36min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'{\"*\"*25}Training model {tuning_configs[3].model_name} on {tuning_configs[3].dataset} {\"*\"*25}')\n",
    "train(tuning_configs[3], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6D3Af1x5tL8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1680811068331,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "p6D3Af1x5tL8",
    "outputId": "3041ed96-a0e7-482d-d754-e8f2522e9fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilization after post training cleanup: \n",
      "CPU RAM Used: 6.9 GB\n",
      "CPU RAM Free: 81.8 GB\n",
      "GPU memory occupied: 8693 MB.\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 3.8 GB\n",
      "Cached:    6.7 GB\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.system('nvidia-smi -caa')\n",
    "print('Utilization after post training cleanup: ')\n",
    "print_utilization()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ueuny-nWmRj",
   "metadata": {
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1680844843596,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "9ueuny-nWmRj"
   },
   "outputs": [],
   "source": [
    "# # Final test list for model trained against s2 dataset.\n",
    "# FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night. She could not sleep at all.',\n",
    "#                    'He stopped himself for a minute and thought if it was the right thing to do. It did seem like a good thing to do.',\n",
    "#                    'There once lived king named Rama. He was very wise and just.',\n",
    "#                    'Once upon a time, an old owl lived in the forest. He was very wise.']\n",
    "\n",
    "# Final test list for model trained against s1 dataset.\n",
    "FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night.',\n",
    "                   'He stopped himself for a minute and thought if it was the right thing to do.',\n",
    "                   'There once lived king named Rama.',\n",
    "                   'Once upon a time, an old owl lived in the forest.']\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, lines, prompt):\n",
    "  transformers.logging.set_verbosity_error()\n",
    "  for test_input_text in lines:\n",
    "      test_inputs = tokenizer([prompt + test_input_text], return_tensors='pt')\n",
    "      test_output_ids = model.generate(\n",
    "          test_inputs['input_ids'].cuda(),\n",
    "          num_beams=5,\n",
    "          no_repeat_ngram_size=3,\n",
    "          num_return_sequences=5,\n",
    "          max_new_tokens=100,\n",
    "          do_sample=True,\n",
    "          top_k=0)\n",
    "      print(f'Input: {test_input_text}')\n",
    "      decoded = [tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for out_ids in test_output_ids]\n",
    "      print(f'Output: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dOsRnXvJMY5C",
   "metadata": {
    "id": "dOsRnXvJMY5C"
   },
   "outputs": [],
   "source": [
    "## Untrained T5 model\n",
    "# evaluate(T5ForConditionalGeneration.from_pretrained(\"t5-large\").cuda(), t5_tokenizer, FINAL_TEST_LIST, \"Continue the next sentence of the story: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "wxim_UyuNkF1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62413,
     "status": "ok",
     "timestamp": 1680845224411,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "wxim_UyuNkF1",
    "outputId": "7adf3b79-3086-4064-e08a-1e8ac89c3d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating google/t5-v1_1-base tuned on s1 dataset\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['Then she sat down by the fire and slept.', '\"It\\'s a dreadful thing,\" she said.', '\"Oh!\" said she, \"what a dreadful thing it is!\"', '\"Ah,\" she said, \"you are a very clever girl, and I am sure you will like it.\"', '\"Is it true?\" asked the Princess, who was in a dreadful state of agitation.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['\"Well,\" he said, \"I\\'m going to tell you what to do.', '\"It\\'s a dream,\" he said, \"but I don\\'t know what it is.', 'Then he sat down and looked at him with astonishment.', 'Then he said, “If you do not like it, I will give you a little supper, and you will be able to eat it and drink it,” and he sat down on a sofa, and ate a bit of bread and a glass of wine.', 'But the first thing he did was to go out into the forest and see if he could find anything to eat.']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['He was a very good man, and he had a good heart.', 'Then he said, “If thou wilt be a king, I will be king.”', \"It was a very lovely palace, with a beautiful garden, a great palace, and a palace of gold and jewels, and the king's daughter, who lived in the palace of the King's son, who was the daughter of a king.\", 'He was a very good-natured man, and he had a great deal of money in his pocket, but he did not know what to do with it.', \"The king was a king, and his son was king of a kingdom, and he lived in a great palace, where he had a daughter and a son, who lived in the palace of the king's daughter.\"]\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['The owl was a great wretch, and he was very angry.', 'The owl was a very good little fellow, and he wore a long black cloak and a large hat.', 'The owl sat in a tree, and when he saw the wren, he said, “It is a wretched man, and he must be killed.”', 'Then the owl sat down, and said: \"It\\'s all right, and I\\'m going to go and see if I can find a good place to sleep in.\"', 'The owl sat on the sand, and said, “I am a hare, and if you can help me I shall be able to help you.”']\n",
      "Evaluating google/t5-v1_1-base tuned on s2 dataset\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['She was a very beautiful Princess, and she loved her dear Prince with all her heart, and her heart was full of love for him.', 'She had slept a long time, and then she was awakened by the sound of a harpooneer’s bell.', 'She was very pale and pale, but she had a heart of gold in her hand and a golden hair on her forehead.', 'She was a frightened girl, and sat down on her bed and listened to the sound of the snorting of a bell.', 'She sat down on the bed and slept for a long time.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['He thought it was a pity that he did not go to bed, and he felt that if he had gone to sleep he would not have been able to find out what he was doing.', 'He said that he had a good idea of what he would do, and he thought it would be better if he could go and see what it was like.', 'He sat down and listened.', '“I don’t know,” he said.', '\"I don\\'t know,\" he said.']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['He had a wife, and a daughter.', 'He was a very wise man, and he was very wise.', 'He was a very wise man, and he knew that if a king had a son, he would be king.', 'He was a very rich man, and he had a son who was the son of a king.', 'He had a wife, a daughter, and a son.']\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['The owl sat on a branch of a tree.', 'The old owl was a very good bird, and was very good-natured.', 'When the owl heard this, he shook his head, and said: “Oh, dear, what a pity it is!', 'The owl had a great deal of money, and he was very fond of it.', 'He was a very clever little bird, and he had a great deal of knowledge about owls.']\n",
      "Evaluating google/t5-v1_1-base tuned on s3 dataset\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['\"I don\\'t know if it\\'s true,\" said the prince.', '“I don’t know,” he said.', '\"It\\'s a fine thing,\" said he, \"but I\\'m afraid he won\\'t be able to get out of it.\"', '“What are you going to do with me?”', '\"It\\'s a pity,\" he said, \"that I don\\'t know what you\\'re going to do.\"']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['“What is it?” cried he, with a sigh.', '\"It\\'s true,\" said the old man, \"but I don\\'t know what it is.', '“It’s a dreadful place,” said he, “and I don’t know what it is.', '“No, sir,” said he.', '\"Oh, yes, if you don\\'t mind, I\\'m going to tell you,\" said the old man, \"and I\\'ll tell you all about it, and you\\'ll be able to see what\\'s going on.\"']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['Then he sat down on his knees, and said, “I am not a thief, but a good man, and a very good man.', '“It’s a very bad thing,” he said, “but if I don’t know what to do, I’ll give it to you.”', '“I don’t know.”', '“Then I’ll go to the top of the hill, and see if he’ll give me a bit of bread.”', '“Well, I don’t know,” he said.']\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['\"It\\'s a pretty place,\" he said.', 'Then the hare said: “I am a king, and if you will give me my kingdom I will give you my kingdom.”', '\"I don\\'t know what to do,\" he said.', '“It’s a long time since I had seen him, and I’ve never seen him since he was a little boy.”', 'Then he sat down on his knees and shook his head.']\n",
      "Evaluating facebook/opt-350m tuned on s2 dataset\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['Princess Leia lay upon her bed all the night.\\nShe was so tired, she was so sleepy.\\nIt was almost midnight, and she was still asleep.\\nI didn\\'t know what to do with myself, but I didn\\'t want to wake her up.\\nSo I went to the bathroom, and then I went back to sleep.\\nWhen I woke up, I looked over at her and said, \"Princess, what are you doing?\"\\nShe looked at me, and her eyes went wide.\\nThen she said,', \"Princess Leia lay upon her bed all the night.\\nIt was a dream.\\nI woke up and she was gone.\\nShe was gone, and I was alone.\\nAnd I couldn't figure out why.\\nThe next morning, I went to the beach.\\nThere was nothing there.\\nNothing.\\nSo I went back to sleep.\\nThen I woke up.\\nWhen I opened my eyes, she was nowhere to be found.\\nThat's when I realized she wasn't dead.\\nBut I didn't know what\", \"Princess Leia lay upon her bed all the night.\\nWell, you know what I mean.\\nI know what you mean, too.\\nWhat do you mean?\\nI mean, what do you think I meant?\\nYou know, I don't know.\\nIt's just...\\nI don't get it.\\nAll right, I'll explain it to you.\\nLet's just say I'm not a fan of Princess Leia.\\nOh, no, you don't.\\nYes, you do.\\nShe's a\", 'Princess Leia lay upon her bed all the night.\\nPrincess Star Wars.', 'Princess Leia lay upon her bed all the night.\\nIt was a dream.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['He stopped himself for a minute and thought if it was the right thing to do.', 'He stopped himself for a minute and thought if it was the right thing to do.', 'He stopped himself for a minute and thought if it was the right thing to do.', \"He stopped himself for a minute and thought if it was the right thing to do.\\nWell, that's how it goes sometimes.\", \"He stopped himself for a minute and thought if it was the right thing to do.\\nHe's not wrong.\\nI don't know how to feel about this.\"]\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['There once lived king named Rama.\\nRama was a very good king.\\nHe was a good king, but he was also a very bad king.', 'There once lived king named Rama.\\nRama was a wise man.', 'There once lived king named Rama.\\nRama was a king who had a wife.\\nHe had a mistress, a daughter, and a son.\\nAnd he had a daughter.\\nHis wife was a beautiful woman, and his son was a handsome man.\\nThe man was tall and handsome, and the daughter was beautiful.\\nBut the daughter had a very strange name.\\nShe was called Sita.\\nSita was a daughter of King Rama, and she was a very beautiful woman.\\nKing R', 'There once lived king named Rama.\\nRama was a good man.\\nHe was a wise man and a good king.', 'There once lived king named Rama.\\nHe was a good man, but he had a heart of gold.']\n",
      "Input: Once upon a time, an old owl lived in the forest.\n",
      "Output: ['Once upon a time, an old owl lived in the forest. He was a good owl. He liked to hunt. He loved to sleep, and he loved to wake up. When he woke up, he looked around and saw that the forest was full of owls. He decided to go back to sleep.\\n\\nWhen he awoke, he saw that there were owls all around him. They were all staring at him. He took a deep breath and said, “I’m going to sleep now.”\\n\\nThe owls', 'Once upon a time, an old owl lived in the forest.\\nHe was a little boy, and he loved to play.\\nOne day, when he was a baby, he fell asleep in a tree.\\nWhen he woke up, he realized that he had fallen asleep on the ground.\\nSo he crawled out of the tree and started to run.\\nAs he ran, he saw a pair of owls.\\nThe owls were very tall, and they were very close to him.\\nThey could see him from far away, but they', \"Once upon a time, an old owl lived in the forest. He had a short tail, and he loved to snuggle with his fur. When he was a child, he used to lie on his back and listen to the sounds of the forest and the birds. He loved to watch the birds fly by, but he didn't like to be near them, so he would lie on the ground and listen for the birds to fly by. When they did, he would look up to see if they were still there, and then he would go back to his\", \"Once upon a time, an old owl lived in the forest.\\nHe was an old man, and he was very fond of his owl.\\nOne day, he fell in love with the owl, and she became his wife.\\nBut the owl didn't love her very much.\\nShe was a very beautiful owl, but she didn't like him very much either.\\nSo the owl gave up his love of the owl and went back to the forest, where he met another owl who was very much like him.\\nThe owl was very happy to\", 'Once upon a time, an old owl lived in the forest.\\nThis owl was a very old owl.\\nHe was very old, and he was very smart.\\nThe old owl was smart, but he was also very old.\\nSo the old owl decided to live in the woods.\\nAnd he lived there for a while.\\nBut then he died.\\nThen he went to the woods and lived there, and then he went back to the forest, and there he lived for a long time, until he died again.\\nWhen he died']\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(tuning_configs):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "  print(f'Evaluating {config.model_name} tuned on {config.dataset} dataset')\n",
    "  if i < 3:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(config.tuned_model_path).to(device)\n",
    "  else:\n",
    "    model = OPTForCausalLM.from_pretrained(config.model_name).to(device)\n",
    "  evaluate(model, tokenizer, FINAL_TEST_LIST, config.prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qy0KM1n3pHnE",
   "metadata": {
    "id": "qy0KM1n3pHnE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1qH9or9UvtJOb_mlSSHcRjZKwZT9uDwbl",
     "timestamp": 1680142704332
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
