{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wHUaO6-oCKak",
   "metadata": {
    "id": "wHUaO6-oCKak"
   },
   "source": [
    "# Manual Annotation\n",
    "This notebook is for running the manual annotation. It supports two activities:\n",
    "-  Identifying the best response in each batch of generated text.\n",
    "- Annotating the best response, as identified by the model, from each batch.\n",
    "- Annotating the best response, as identified manually in the above step from each batch.\n",
    "\n",
    "The first step of manually identifying the best response should be done just once by one person while the next step should be done individually by each annotator. Before getting started, ensure that the annotator name is set correctly.\n",
    "\n",
    "Pre-requisites:\n",
    "- The annotation input file, generated through the inferencing.ipynb notebook, must be available in GDRIVE_BASE location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97nY_nL9CeiJ",
   "metadata": {
    "id": "97nY_nL9CeiJ"
   },
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "IoJvS_GSj2AZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoJvS_GSj2AZ",
    "outputId": "c1e3157a-d7db-4474-ebd5-68d3194a6a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting krippendorff\n",
      "  Downloading krippendorff-0.6.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from krippendorff) (1.22.4)\n",
      "Installing collected packages: krippendorff\n",
      "Successfully installed krippendorff-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "egOvt-Oq0M2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "egOvt-Oq0M2J",
    "outputId": "a878cb72-4834-4907-eeb5-5181198db1c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from  nltk.metrics import agreement\n",
    "import krippendorff\n",
    "\n",
    "GDRIVE_BASE = 'drive/MyDrive/MIDS/w266/project/'\n",
    "# ANNOTATOR='nico'\n",
    "# ANNOTATOR='ghiwa'\n",
    "ANNOTATOR='ram'\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "sys.path.insert(0, GDRIVE_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RIepPh55dtNF",
   "metadata": {
    "id": "RIepPh55dtNF"
   },
   "source": [
    "# Best in Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pTF_5WE-d7rv",
   "metadata": {
    "id": "pTF_5WE-d7rv"
   },
   "outputs": [],
   "source": [
    "BIB_IN_FILE = f'{GDRIVE_BASE}manual_eval_generated_output.csv'\n",
    "BIB_OUT_FILE = f'{GDRIVE_BASE}manual_eval_generated_output_with_bib.csv'\n",
    "\n",
    "def is_completed(batch):\n",
    "  return len(batch[batch.human_top_score == True]) > 0\n",
    "\n",
    "def mark_best(ignore_completed=False, save_each_step=True):\n",
    "  # Load the annotation input\n",
    "  if ignore_completed:\n",
    "    # Load from prevously saved work.\n",
    "    df_results = pd.read_csv(BIB_OUT_FILE)\n",
    "  else:\n",
    "    # Load from the original annotation input.\n",
    "    df_results = pd.read_csv(BIB_IN_FILE)\n",
    "    # Add a new column to hold the best in batch flag.\n",
    "    df_results['human_top_score'] = False\n",
    "  num_batches = df_results.batch_id.max()\n",
    "  batch_size = int(df_results.shape[0] / num_batches)\n",
    "\n",
    "  # Random order of batches.\n",
    "  batch_ids = np.arange(num_batches)\n",
    "  np.random.shuffle(batch_ids)\n",
    "  with tqdm(total=num_batches, unit='item', unit_scale=True) as pbar:\n",
    "    for cur_batch_id in batch_ids:\n",
    "      # Adjust for batch ids starting from 1.\n",
    "      cur_batch_id = cur_batch_id + 1\n",
    "      batch = df_results[df_results.batch_id == cur_batch_id]\n",
    "      if len(batch) != batch_size:\n",
    "        # We should never get here something is wrong.\n",
    "        raise Exception(f'Batch {cur_batch_id} has {len(batch)} items but expecting {batch_size} items.')\n",
    "      if is_completed(batch):\n",
    "        print(f'Skipping batch {cur_batch_id} as it is already completed.')\n",
    "      else:\n",
    "        cur_prompt = batch.prompt.iloc[0]\n",
    "        pbar.set_postfix(batch_id=cur_batch_id, prompt=cur_prompt[0:10] + '...', refresh=True)\n",
    "        # Display the batch and get user's choice pick the best.\n",
    "        indices = {}\n",
    "        print(f'PROMPT: {cur_prompt}')\n",
    "        i = 1\n",
    "        for index, row in batch.iterrows():\n",
    "          cur_generated = row[\"generated\"] #.replace(cur_prompt + ' ', '')\n",
    "          print(f'{i}. {cur_generated}')\n",
    "          indices[i] = index\n",
    "          i = i+1\n",
    "        \n",
    "        user_opt = -1\n",
    "        msg = f'Enter 1 to {batch_size} or \"quit\"'\n",
    "        quit = False\n",
    "        while user_opt == -1:\n",
    "          try:\n",
    "            user_input = input(f'{msg}: ')\n",
    "            if user_input == 'quit':\n",
    "              quit = True\n",
    "              break\n",
    "            user_opt = int(user_input)\n",
    "            if user_opt in indices:\n",
    "              df_results.loc[indices[user_opt],'human_top_score'] = True\n",
    "              if save_each_step:\n",
    "                df_results.to_csv(BIB_OUT_FILE, index=None)\n",
    "            else:\n",
    "              print(msg)\n",
    "              user_opt = -1\n",
    "          except ValueError:\n",
    "            print(msg)\n",
    "            user_opt = -1      \n",
    "        pbar.update(1)\n",
    "        if quit:\n",
    "          break\n",
    "    df_results.to_csv(BIB_OUT_FILE, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ZqiupvHh7VmF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "63b4026554664daa8424bb17fa4a7deb",
      "f5dcd73bb4944b60a8f08bf237323f0c",
      "68209ef35c66450388239b85c6df934a",
      "20aa637ca60041f0b478d320d273be48",
      "3b2c0864154749e19aba48a70e34c16a",
      "4ae8480630564cceb3373e20f6adfe22",
      "82eb9444bf1a409e89ba2107bf807dba",
      "88f05835d749429a90e4cd0999037d55",
      "13296faa9b2340e180389851a8e4af94",
      "bbfa2260acd945fbbbc9abd5a1fd243d",
      "97246a6d65b3404db3d72fac3e69596d"
     ]
    },
    "id": "ZqiupvHh7VmF",
    "outputId": "33947e1f-84d7-4046-cfeb-c6dfbc17da21"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b4026554664daa8424bb17fa4a7deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/120 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 48 as it is already completed.\n",
      "Skipping batch 13 as it is already completed.\n",
      "Skipping batch 3 as it is already completed.\n",
      "Skipping batch 90 as it is already completed.\n",
      "Skipping batch 100 as it is already completed.\n",
      "Skipping batch 2 as it is already completed.\n",
      "Skipping batch 104 as it is already completed.\n",
      "Skipping batch 32 as it is already completed.\n",
      "Skipping batch 28 as it is already completed.\n",
      "Skipping batch 69 as it is already completed.\n",
      "Skipping batch 46 as it is already completed.\n",
      "Skipping batch 102 as it is already completed.\n",
      "Skipping batch 8 as it is already completed.\n",
      "Skipping batch 88 as it is already completed.\n",
      "Skipping batch 94 as it is already completed.\n",
      "Skipping batch 114 as it is already completed.\n",
      "PROMPT: \"There's no going back now Mr. Brandon, what should we do?\" Penny asked desperately.\n",
      "1. \" I don ' t know, \" said Mrs. March.\n",
      "2. \" I don ' t know, \" said Mrs. Watkinson.\n",
      "3. \" I don ' t know, \" said Mrs. Moffat.\n",
      "4. \" No, I don ' t.\n",
      "5. nan\n",
      "Enter 1 to 5 or \"quit\": 1\n",
      "Skipping batch 120 as it is already completed.\n",
      "Skipping batch 12 as it is already completed.\n",
      "Skipping batch 115 as it is already completed.\n",
      "Skipping batch 91 as it is already completed.\n",
      "Skipping batch 74 as it is already completed.\n",
      "Skipping batch 6 as it is already completed.\n",
      "Skipping batch 70 as it is already completed.\n",
      "Skipping batch 76 as it is already completed.\n",
      "Skipping batch 51 as it is already completed.\n",
      "Skipping batch 43 as it is already completed.\n",
      "Skipping batch 111 as it is already completed.\n",
      "Skipping batch 50 as it is already completed.\n",
      "Skipping batch 17 as it is already completed.\n",
      "Skipping batch 83 as it is already completed.\n",
      "Skipping batch 33 as it is already completed.\n",
      "Skipping batch 21 as it is already completed.\n",
      "Skipping batch 18 as it is already completed.\n",
      "Skipping batch 80 as it is already completed.\n",
      "Skipping batch 42 as it is already completed.\n",
      "Skipping batch 9 as it is already completed.\n",
      "Skipping batch 108 as it is already completed.\n",
      "Skipping batch 98 as it is already completed.\n",
      "Skipping batch 110 as it is already completed.\n",
      "Skipping batch 47 as it is already completed.\n",
      "Skipping batch 79 as it is already completed.\n",
      "Skipping batch 54 as it is already completed.\n",
      "Skipping batch 37 as it is already completed.\n",
      "Skipping batch 82 as it is already completed.\n",
      "Skipping batch 1 as it is already completed.\n",
      "Skipping batch 118 as it is already completed.\n",
      "Skipping batch 117 as it is already completed.\n",
      "Skipping batch 11 as it is already completed.\n",
      "Skipping batch 71 as it is already completed.\n",
      "Skipping batch 57 as it is already completed.\n",
      "Skipping batch 23 as it is already completed.\n",
      "Skipping batch 49 as it is already completed.\n",
      "Skipping batch 64 as it is already completed.\n",
      "Skipping batch 77 as it is already completed.\n",
      "Skipping batch 29 as it is already completed.\n",
      "Skipping batch 39 as it is already completed.\n",
      "Skipping batch 99 as it is already completed.\n",
      "Skipping batch 106 as it is already completed.\n",
      "Skipping batch 97 as it is already completed.\n",
      "Skipping batch 73 as it is already completed.\n",
      "Skipping batch 72 as it is already completed.\n",
      "Skipping batch 66 as it is already completed.\n",
      "Skipping batch 14 as it is already completed.\n",
      "Skipping batch 30 as it is already completed.\n",
      "Skipping batch 34 as it is already completed.\n",
      "Skipping batch 58 as it is already completed.\n",
      "Skipping batch 26 as it is already completed.\n",
      "Skipping batch 87 as it is already completed.\n",
      "Skipping batch 105 as it is already completed.\n",
      "Skipping batch 61 as it is already completed.\n",
      "Skipping batch 41 as it is already completed.\n",
      "Skipping batch 31 as it is already completed.\n",
      "Skipping batch 78 as it is already completed.\n",
      "Skipping batch 113 as it is already completed.\n",
      "Skipping batch 4 as it is already completed.\n",
      "Skipping batch 56 as it is already completed.\n",
      "Skipping batch 84 as it is already completed.\n",
      "Skipping batch 5 as it is already completed.\n",
      "Skipping batch 36 as it is already completed.\n",
      "Skipping batch 65 as it is already completed.\n",
      "Skipping batch 19 as it is already completed.\n",
      "Skipping batch 93 as it is already completed.\n",
      "Skipping batch 103 as it is already completed.\n",
      "Skipping batch 119 as it is already completed.\n",
      "Skipping batch 67 as it is already completed.\n",
      "Skipping batch 52 as it is already completed.\n",
      "Skipping batch 86 as it is already completed.\n",
      "Skipping batch 20 as it is already completed.\n",
      "Skipping batch 68 as it is already completed.\n",
      "Skipping batch 95 as it is already completed.\n",
      "Skipping batch 96 as it is already completed.\n",
      "Skipping batch 16 as it is already completed.\n",
      "Skipping batch 101 as it is already completed.\n",
      "Skipping batch 45 as it is already completed.\n",
      "Skipping batch 38 as it is already completed.\n",
      "Skipping batch 10 as it is already completed.\n",
      "Skipping batch 81 as it is already completed.\n",
      "Skipping batch 116 as it is already completed.\n",
      "Skipping batch 63 as it is already completed.\n",
      "Skipping batch 62 as it is already completed.\n",
      "Skipping batch 109 as it is already completed.\n",
      "Skipping batch 85 as it is already completed.\n",
      "Skipping batch 24 as it is already completed.\n",
      "Skipping batch 40 as it is already completed.\n",
      "Skipping batch 60 as it is already completed.\n",
      "Skipping batch 7 as it is already completed.\n",
      "Skipping batch 27 as it is already completed.\n",
      "Skipping batch 92 as it is already completed.\n",
      "Skipping batch 35 as it is already completed.\n",
      "Skipping batch 25 as it is already completed.\n",
      "Skipping batch 53 as it is already completed.\n",
      "Skipping batch 59 as it is already completed.\n",
      "Skipping batch 107 as it is already completed.\n",
      "Skipping batch 22 as it is already completed.\n",
      "Skipping batch 75 as it is already completed.\n",
      "Skipping batch 112 as it is already completed.\n",
      "Skipping batch 55 as it is already completed.\n",
      "Skipping batch 44 as it is already completed.\n",
      "Skipping batch 89 as it is already completed.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to start an interactive session to identify best generated text in each batch.\n",
    "# This will walk through the batches randomly and present one batch at a time. \n",
    "# A batch is defined as the N (currently N=5) texts generated from one model configuration for one prompt.\n",
    "# For each batch shown, select the best text. The method creates a new file with the final result.\n",
    "# Set ignore_completed to True if continuing from a previous session.\n",
    "# save_each_step=True ensures that the dataframe is saved after each batch is marked.\n",
    "mark_best(ignore_completed=True, save_each_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0XNT9FzpKUJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "f0XNT9FzpKUJ",
    "outputId": "071a1796-47c6-4161-c863-e3e97a1cd0be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIB annotation completed for 119 of 120 entries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6a581037-d043-43ac-9949-411eb8b7b64d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "      <th>tuned</th>\n",
       "      <th>dataset</th>\n",
       "      <th>prompt</th>\n",
       "      <th>score</th>\n",
       "      <th>generated</th>\n",
       "      <th>model_top_score</th>\n",
       "      <th>human_top_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>t5_s3</td>\n",
       "      <td>t5-v1_1-base-s3-finetuned</td>\n",
       "      <td>True</td>\n",
       "      <td>s3</td>\n",
       "      <td>My stomach did a flip, then a flop, I couldn't...</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>I thought of the money, but then I saw another...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>t5_s3</td>\n",
       "      <td>t5-v1_1-base-s3-finetuned</td>\n",
       "      <td>True</td>\n",
       "      <td>s3</td>\n",
       "      <td>My stomach did a flip, then a flop, I couldn't...</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>\"Come here, come here!\" cried John.</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>t5_s3</td>\n",
       "      <td>t5-v1_1-base-s3-finetuned</td>\n",
       "      <td>True</td>\n",
       "      <td>s3</td>\n",
       "      <td>My stomach did a flip, then a flop, I couldn't...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>I went to the hole and looked at it, and drew ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>t5_s3</td>\n",
       "      <td>t5-v1_1-base-s3-finetuned</td>\n",
       "      <td>True</td>\n",
       "      <td>s3</td>\n",
       "      <td>My stomach did a flip, then a flop, I couldn't...</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>I sprang back in the water and threw my hands ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>t5_s3</td>\n",
       "      <td>t5-v1_1-base-s3-finetuned</td>\n",
       "      <td>True</td>\n",
       "      <td>s3</td>\n",
       "      <td>My stomach did a flip, then a flop, I couldn't...</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>I sat up and looked.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>120</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>0.017286</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>120</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>120</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>0.017263</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>120</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>0.026418</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>120</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>0.018153</td>\n",
       "      <td>Do you think my mom will let me have pizza for...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 10 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a581037-d043-43ac-9949-411eb8b7b64d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6a581037-d043-43ac-9949-411eb8b7b64d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6a581037-d043-43ac-9949-411eb8b7b64d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     batch_id      name                      model  tuned dataset  \\\n",
       "0           1     t5_s3  t5-v1_1-base-s3-finetuned   True      s3   \n",
       "1           1     t5_s3  t5-v1_1-base-s3-finetuned   True      s3   \n",
       "2           1     t5_s3  t5-v1_1-base-s3-finetuned   True      s3   \n",
       "3           1     t5_s3  t5-v1_1-base-s3-finetuned   True      s3   \n",
       "4           1     t5_s3  t5-v1_1-base-s3-finetuned   True      s3   \n",
       "..        ...       ...                        ...    ...     ...   \n",
       "595       120  baseline          facebook/opt-350m  False     NaN   \n",
       "596       120  baseline          facebook/opt-350m  False     NaN   \n",
       "597       120  baseline          facebook/opt-350m  False     NaN   \n",
       "598       120  baseline          facebook/opt-350m  False     NaN   \n",
       "599       120  baseline          facebook/opt-350m  False     NaN   \n",
       "\n",
       "                                                prompt     score  \\\n",
       "0    My stomach did a flip, then a flop, I couldn't...  0.000084   \n",
       "1    My stomach did a flip, then a flop, I couldn't...  0.000063   \n",
       "2    My stomach did a flip, then a flop, I couldn't...  0.000060   \n",
       "3    My stomach did a flip, then a flop, I couldn't...  0.000096   \n",
       "4    My stomach did a flip, then a flop, I couldn't...  0.000183   \n",
       "..                                                 ...       ...   \n",
       "595  Do you think my mom will let me have pizza for...  0.017286   \n",
       "596  Do you think my mom will let me have pizza for...  0.030708   \n",
       "597  Do you think my mom will let me have pizza for...  0.017263   \n",
       "598  Do you think my mom will let me have pizza for...  0.026418   \n",
       "599  Do you think my mom will let me have pizza for...  0.018153   \n",
       "\n",
       "                                             generated  model_top_score  \\\n",
       "0    I thought of the money, but then I saw another...            False   \n",
       "1                  \"Come here, come here!\" cried John.            False   \n",
       "2    I went to the hole and looked at it, and drew ...            False   \n",
       "3    I sprang back in the water and threw my hands ...            False   \n",
       "4                                 I sat up and looked.             True   \n",
       "..                                                 ...              ...   \n",
       "595  Do you think my mom will let me have pizza for...            False   \n",
       "596  Do you think my mom will let me have pizza for...             True   \n",
       "597  Do you think my mom will let me have pizza for...            False   \n",
       "598  Do you think my mom will let me have pizza for...            False   \n",
       "599  Do you think my mom will let me have pizza for...            False   \n",
       "\n",
       "     human_top_score  \n",
       "0              False  \n",
       "1               True  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  \n",
       "..               ...  \n",
       "595            False  \n",
       "596            False  \n",
       "597            False  \n",
       "598             True  \n",
       "599            False  \n",
       "\n",
       "[600 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n"
     ]
    }
   ],
   "source": [
    "# Show the final results with best of batch marked.\n",
    "df_results = pd.read_csv(BIB_OUT_FILE)\n",
    "print(f'BIB annotation completed for {df_results[df_results.human_top_score == True].shape[0]} of {df_results.batch_id.max()} entries.')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qz7ZZm8NFEmp",
   "metadata": {
    "id": "qz7ZZm8NFEmp"
   },
   "source": [
    "# Bug Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4mTs2ZR0FC7v",
   "metadata": {
    "id": "4mTs2ZR0FC7v"
   },
   "outputs": [],
   "source": [
    "# Update annotated file with results from models that were excluded during annotation\n",
    "PRE_ANNOTATION_FILE = f'{GDRIVE_BASE}manual_eval_generated_output_with_bib.csv'\n",
    "POST_ANNOTATION_FILE = f'{GDRIVE_BASE}{ANNOTATOR}_final_annotations.csv'\n",
    "# Backup file name. The script will first create a backup of the annotated file before overwriting it.\n",
    "POST_ANNOTATION_FILE_BACK = f'{GDRIVE_BASE}{ANNOTATOR}_final_annotations_test_backup.csv'\n",
    "import shutil\n",
    "\n",
    "def is_match(cur_gen_id, pre_row, post_row):\n",
    "  def is_col_match(col_name):\n",
    "    is_match = (pre_row[col_name] == post_row[col_name])\n",
    "    return is_match\n",
    "  matching = is_col_match('name') and is_col_match('generated') and is_col_match('prompt') and \\\n",
    "      is_col_match('model_top_score') and is_col_match('human_top_score')\n",
    "  if not matching:\n",
    "    print('================Pre================')\n",
    "    print(pre_row)\n",
    "    print('================Post================')\n",
    "    print(post_row)\n",
    "  return matching\n",
    "\n",
    "def bug_fix_1():\n",
    "  # \n",
    "  # Read the pre-annotation file.\n",
    "  df_pre = pd.read_csv(PRE_ANNOTATION_FILE)\n",
    "  # Add the annotation related cols with defaults\n",
    "  df_pre['gen_id'] = df_pre.index \n",
    "  df_pre['annotated'] = False\n",
    "  for metric in METRICS:\n",
    "    df_pre[metric] = -1\n",
    "\n",
    "  # Iterate through post annotation file and update pre with the data.\n",
    "  df_post = pd.read_csv(POST_ANNOTATION_FILE)\n",
    "  for index, post_row in df_post.iterrows():\n",
    "    cur_gen_id = post_row.gen_id\n",
    "    pre_row = df_pre.loc[cur_gen_id]\n",
    "    if not is_match(cur_gen_id, pre_row, post_row):\n",
    "      raise Exception(f'Rows at {cur_gen_id} are not matching.')\n",
    "    else:\n",
    "      df_pre.loc[cur_gen_id, 'annotated'] = post_row['annotated']\n",
    "      for metric in METRICS:\n",
    "        df_pre.loc[cur_gen_id, metric] = post_row[metric]\n",
    "  \n",
    "  # Backup old data before overwriting.\n",
    "  shutil.copyfile(POST_ANNOTATION_FILE, POST_ANNOTATION_FILE_BACK)\n",
    "  df_post.to_csv(POST_ANNOTATION_FILE_BACK, index=None)\n",
    "  df_pre.to_csv(POST_ANNOTATION_FILE, index=None)\n",
    "  return df_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AqeIKntmJYy-",
   "metadata": {
    "id": "AqeIKntmJYy-"
   },
   "outputs": [],
   "source": [
    "# Un comment the below line to run the fix for missing t5_s3, opt_s3 and b2b_s3 models\n",
    "# during annotation.\n",
    "# bug_fix_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JkzklQ0RjZl6",
   "metadata": {
    "id": "JkzklQ0RjZl6"
   },
   "source": [
    "# Manual Scoring Of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9VdP1mqn1gVg",
   "metadata": {
    "id": "9VdP1mqn1gVg"
   },
   "outputs": [],
   "source": [
    "MA_IN_FILE=f'{GDRIVE_BASE}manual_eval_generated_output_with_bib.csv'\n",
    "MA_OUT_FILE=f'{GDRIVE_BASE}{ANNOTATOR}_final_annotations.csv'\n",
    "METRICS = [\n",
    "    'relevance',\n",
    "    'readability',\n",
    "    'grammaticality',\n",
    "    'non-redundancy',\n",
    "    'kid-friendly']\n",
    "SCALES = [\n",
    "    ('outstanding', 7),\n",
    "    ('very good', 6),\n",
    "    ('good', 5),\n",
    "    ('neutral', 4),\n",
    "    ('bad', 3),\n",
    "    ('very bad', 2),\n",
    "    ('unusable', 1)]\n",
    "\n",
    "def load_data(in_file, out_file, ignore_completed):\n",
    "  if ignore_completed:\n",
    "    df_data = pd.read_csv(out_file)\n",
    "  else:\n",
    "    df_data = pd.read_csv(in_file)\n",
    "    # Add the extra cols for annotation.\n",
    "    df_data['gen_id'] = df_data.index # Explicit id columns as we want to retain \n",
    "                                      # the original indices even after filtering.\n",
    "    df_data['annotated'] = False\n",
    "    for metric in METRICS:\n",
    "      df_data[metric] = -1\n",
    "\n",
    "  # Columns: 'batch_id', 'name', 'model', 'tuned', 'dataset', 'prompt', 'score', 'generated', 'model_top_score', 'human_top_score'\n",
    "  # Pick only entries from models trained on S3 dataset and also only the \n",
    "  # top entries - one of model_top_score, human_top_score is True.\n",
    "  df_filtered = df_data[((df_data.dataset == 's3') | (df_data.name == 'baseline')) & ((df_data.model_top_score) | (df_data.human_top_score))]\n",
    "  # df_filtered = df_data[(df_data.name == 'baseline') & ((df_data.model_top_score) | (df_data.human_top_score))]\n",
    "  return df_filtered\n",
    "\n",
    "def annotate_one(cur_index, cur_row):\n",
    "  print('='*50)\n",
    "  msg = f\"Enter one of {', '.join([f'{scale}({score})' for scale, score in SCALES])}\"\n",
    "  scores = {}\n",
    "  print(msg)\n",
    "  quit = False\n",
    "  # Initialize the map with -1 scores.\n",
    "  for metric in METRICS:\n",
    "    scores[metric] = -1\n",
    "  for metric in METRICS:\n",
    "    # Get score for current metric.\n",
    "    print(f'\\tScore generated text for {metric}: ')\n",
    "    quit = False\n",
    "    user_opt = -1\n",
    "    while user_opt == -1:\n",
    "      try:\n",
    "        user_input = input(f'Your score (or \"quit\"): ')\n",
    "        if user_input == 'quit':\n",
    "          quit = True\n",
    "          return quit, scores\n",
    "        user_opt = int(user_input)\n",
    "        if user_opt >= 1 and  user_opt <= 7:\n",
    "          scores[metric] = user_opt\n",
    "        else:\n",
    "          print(f'Invalid entry. Please {msg}')\n",
    "          user_opt = -1\n",
    "      except ValueError:\n",
    "        print(msg)\n",
    "        user_opt = -1\n",
    "  return quit, scores\n",
    "\n",
    "def annotate(annotator, in_file, out_file, ignore_completed=False, save_each_step=True):\n",
    "  df_data = load_data(in_file, out_file, ignore_completed)\n",
    "  num_entries = df_data.shape[0]\n",
    "  print(f'Total annotation batch size: {num_entries}')\n",
    "\n",
    "  # Random order of items.\n",
    "  indices = df_data.index.values.copy()\n",
    "  np.random.shuffle(indices)\n",
    "  print(f'Score scale: ')\n",
    "  with tqdm(total=indices.shape[0], unit='item', unit_scale=True) as pbar:\n",
    "    for cur_index in indices:\n",
    "      cur_row = df_data.loc[cur_index]\n",
    "      cur_prompt = cur_row['prompt']\n",
    "      cur_generated = cur_row['generated']\n",
    "      cur_generated = cur_generated.replace(cur_prompt + ' ', '')\n",
    "      if ignore_completed and cur_row['annotated']:\n",
    "        print(f'Skipping row {cur_index} as it is already annotated.')\n",
    "      else:\n",
    "        # Display the prompt and the generated text.\n",
    "        pbar.set_postfix(prompt=cur_prompt[0:10] + '...', generated=cur_generated[0:10] + '...', refresh=True)\n",
    "        print(f'PROMPT: {cur_prompt}')\n",
    "        print(f'GENERATED: {cur_generated}')\n",
    "        quit, scores = annotate_one(cur_index, cur_row)\n",
    "        pbar.update(1)\n",
    "        if quit:\n",
    "          break\n",
    "        df_data.loc[cur_index, 'annotated'] = True\n",
    "        for metric in METRICS:\n",
    "          df_data.loc[cur_index, metric] = scores[metric]\n",
    "        if save_each_step:\n",
    "          df_data.to_csv(out_file, index=None)\n",
    "    df_data.to_csv(out_file, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "TQFII8savCS4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c880775943d34b6d89c09b1f97b655d9",
      "41a69b93db4b410cb58e26f93611301b",
      "3a156101dce9491a88fefa086d9a983d",
      "6678d182c6704a589f84357243b5cd3c",
      "9412a99fbebd46208a1fe868a8d24d8d",
      "c4df92718cbf466c807320ff7aacebf8",
      "0c020c6dac3144cf8a10e9df46855584",
      "86954469aef54301a8d8bcf1c149fe63",
      "68863301a7c841ea9a2b445689f6188a",
      "06c4bb8ba6cf46e0bb70a307432f380d",
      "8a31093ea2774266bb1cc0edfa989052"
     ]
    },
    "id": "TQFII8savCS4",
    "outputId": "7c7924b2-e647-4d11-8f54-ccd8dfb750e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotation batch size: 4\n",
      "Score scale: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c880775943d34b6d89c09b1f97b655d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/4.00 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Lara felt very sad and scared.\n",
      "GENERATED: Continue the next sentence of the story making the language appropriate for kids between 6 and 12 years old: And that's it.  She started crying. She wanted to leave. But she had to stay.\n",
      "==================================================\n",
      "Enter one of outstanding(7), very good(6), good(5), neutral(4), bad(3), very bad(2), unusable(1)\n",
      "\tScore generated text for relevance: \n",
      "Your score (or \"quit\"): 7\n",
      "\tScore generated text for readability: \n",
      "Your score (or \"quit\"): 7\n",
      "\tScore generated text for grammaticality: \n",
      "Your score (or \"quit\"): 7\n",
      "\tScore generated text for non-redundancy: \n",
      "Your score (or \"quit\"): 7\n",
      "\tScore generated text for kid-friendly: \n",
      "Your score (or \"quit\"): 7\n",
      "PROMPT: All the dragons of the world lived on one mountain called the dragon mountain.\n",
      "GENERATED: Continue the next sentence of the story making the language appropriate for kids between 6 and 12 years old: It was a mountain on which all dragons lived. The dragons that lived\n",
      "==================================================\n",
      "Enter one of outstanding(7), very good(6), good(5), neutral(4), bad(3), very bad(2), unusable(1)\n",
      "\tScore generated text for relevance: \n",
      "Your score (or \"quit\"): 6\n",
      "\tScore generated text for readability: \n",
      "Your score (or \"quit\"): 6\n",
      "\tScore generated text for grammaticality: \n",
      "Your score (or \"quit\"): 6\n",
      "\tScore generated text for non-redundancy: \n",
      "Your score (or \"quit\"): 6\n",
      "\tScore generated text for kid-friendly: \n",
      "Your score (or \"quit\"): 6\n",
      "PROMPT: All the dragons of the world lived on one mountain called the dragon mountain.\n",
      "GENERATED: Continue the next sentence of the story making the language appropriate for kids between 6 and 12 years old: The dragon is a dragon that lived in the mountains of North America\n",
      "==================================================\n",
      "Enter one of outstanding(7), very good(6), good(5), neutral(4), bad(3), very bad(2), unusable(1)\n",
      "\tScore generated text for relevance: \n",
      "Your score (or \"quit\"): 1\n",
      "\tScore generated text for readability: \n",
      "Your score (or \"quit\"): 2\n",
      "\tScore generated text for grammaticality: \n",
      "Your score (or \"quit\"): 1\n",
      "\tScore generated text for non-redundancy: \n",
      "Your score (or \"quit\"): 2\n",
      "\tScore generated text for kid-friendly: \n",
      "Your score (or \"quit\"): 1\n",
      "PROMPT: Lara felt very sad and scared.\n",
      "GENERATED: Continue the next sentence of the story making the language appropriate for kids between 6 and 12 years old:  \"She was crying for no reason. She was not crying because she was scared, she wasn't\n",
      "==================================================\n",
      "Enter one of outstanding(7), very good(6), good(5), neutral(4), bad(3), very bad(2), unusable(1)\n",
      "\tScore generated text for relevance: \n",
      "Your score (or \"quit\"): 5\n",
      "\tScore generated text for readability: \n",
      "Your score (or \"quit\"): 5\n",
      "\tScore generated text for grammaticality: \n",
      "Your score (or \"quit\"): 5\n",
      "\tScore generated text for non-redundancy: \n",
      "Your score (or \"quit\"): 5\n",
      "\tScore generated text for kid-friendly: \n",
      "Your score (or \"quit\"): 5\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the follwing variables are set correctly:\n",
    "#   ANNOTATOR - name of the annotator\n",
    "#   MA_IN_FILE - must be the file created through the \"Best In Batch\" section above.\n",
    "#   MA_OUT_FILE - output file that will hold annotations. (Please keep the annotator name in the filename).\n",
    "# Run this cell to start an interactive session to annotate each generated text.\n",
    "# This will walk through the generated texts randomly and present one text at a time. \n",
    "# It will prompt you to evaluate on one metric at a time. Each metric is evaluated on a scale of 1 to 7 with 1 being \"Unusable\" and 7 being \"Outstanding\".\n",
    "# You can quit any time by entering \"quit\" at a prompt. Partial scores will be ignored if you enter quit. \n",
    "# If you want to restart from where you left off then set ignore_completed to True.\n",
    "# Ensure that save_each_step=True so that no data is lost.\n",
    "# Finally the process saves annotations in a file specific to ANNOTATOR.\n",
    "\n",
    "annotate(ANNOTATOR, MA_IN_FILE, MA_OUT_FILE, ignore_completed=True, save_each_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "v7SfXUHnRRGo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "v7SfXUHnRRGo",
    "outputId": "11105924-eac1-477c-a6a7-267593a87c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual scoring completed for 4 of 4 entries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-54353220-e1cc-464f-b2cc-81c6d511d268\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "      <th>tuned</th>\n",
       "      <th>dataset</th>\n",
       "      <th>prompt</th>\n",
       "      <th>score</th>\n",
       "      <th>generated</th>\n",
       "      <th>model_top_score</th>\n",
       "      <th>human_top_score</th>\n",
       "      <th>gen_id</th>\n",
       "      <th>annotated</th>\n",
       "      <th>relevance</th>\n",
       "      <th>readability</th>\n",
       "      <th>grammaticality</th>\n",
       "      <th>non-redundancy</th>\n",
       "      <th>kid-friendly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lara felt very sad and scared.</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>Continue the next sentence of the story making...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lara felt very sad and scared.</td>\n",
       "      <td>0.011939</td>\n",
       "      <td>Continue the next sentence of the story making...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All the dragons of the world lived on one moun...</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>Continue the next sentence of the story making...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>baseline</td>\n",
       "      <td>facebook/opt-350m</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All the dragons of the world lived on one moun...</td>\n",
       "      <td>0.006506</td>\n",
       "      <td>Continue the next sentence of the story making...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>88</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54353220-e1cc-464f-b2cc-81c6d511d268')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-54353220-e1cc-464f-b2cc-81c6d511d268 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-54353220-e1cc-464f-b2cc-81c6d511d268');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   batch_id      name              model  tuned  dataset  \\\n",
       "0         9  baseline  facebook/opt-350m  False      NaN   \n",
       "1         9  baseline  facebook/opt-350m  False      NaN   \n",
       "2        18  baseline  facebook/opt-350m  False      NaN   \n",
       "3        18  baseline  facebook/opt-350m  False      NaN   \n",
       "\n",
       "                                              prompt     score  \\\n",
       "0                     Lara felt very sad and scared.  0.001480   \n",
       "1                     Lara felt very sad and scared.  0.011939   \n",
       "2  All the dragons of the world lived on one moun...  0.006471   \n",
       "3  All the dragons of the world lived on one moun...  0.006506   \n",
       "\n",
       "                                           generated  model_top_score  \\\n",
       "0  Continue the next sentence of the story making...            False   \n",
       "1  Continue the next sentence of the story making...             True   \n",
       "2  Continue the next sentence of the story making...            False   \n",
       "3  Continue the next sentence of the story making...             True   \n",
       "\n",
       "   human_top_score  gen_id  annotated  relevance  readability  grammaticality  \\\n",
       "0             True      40       True          5            5               5   \n",
       "1            False      42       True          7            7               7   \n",
       "2             True      85       True          1            2               1   \n",
       "3            False      88       True          6            6               6   \n",
       "\n",
       "   non-redundancy  kid-friendly  \n",
       "0               5             5  \n",
       "1               7             7  \n",
       "2               2             1  \n",
       "3               6             6  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotated = pd.read_csv(MA_OUT_FILE)\n",
    "print(f'Manual scoring completed for {df_annotated[df_annotated.annotated == True].shape[0]} of {df_annotated.shape[0]} entries.')\n",
    "\n",
    "df_annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usJ89qZj6y3i",
   "metadata": {
    "id": "usJ89qZj6y3i"
   },
   "source": [
    "# Score Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "VHyPCy3663BU",
   "metadata": {
    "id": "VHyPCy3663BU"
   },
   "outputs": [],
   "source": [
    "ANNOTATORS = ['ghiwa', 'nico', 'ram']\n",
    "ANNOTATION_FILE_FORMAT = '{}{}_final_annotations_test.csv' # Needs GDRIVE_BASE and annotator.\n",
    "FINAL_SCORE_SUMMARY_FILE=f'{GDRIVE_BASE}model_annotator_metric_mean.csv'\n",
    "MODELS = ['t5_s3', 'opt_s3', 'b2b_s3', 'baseline']\n",
    "METRICS = [\n",
    "    'relevance',\n",
    "    'readability',\n",
    "    'grammaticality',\n",
    "    'non-redundancy',\n",
    "    'kid-friendly']\n",
    "SCALES = [\n",
    "    ('outstanding', 7),\n",
    "    ('very good', 6),\n",
    "    ('good', 5),\n",
    "    ('neutral', 4),\n",
    "    ('bad', 3),\n",
    "    ('very bad', 2),\n",
    "    ('unusable', 1)]\n",
    "\n",
    "def scores_for_krippendorff(annotator, annotations, human_top_score, metric):\n",
    "  if human_top_score:\n",
    "    df_filtered = annotations[(annotations.human_top_score == True)]\n",
    "  else:\n",
    "    df_filtered = annotations[(annotations.model_top_score == True)]\n",
    "  if metric is None:\n",
    "    scores = []\n",
    "    for metric in METRICS:\n",
    "      scores.extend(list(df_filtered[[metric]].values.flat))\n",
    "      return scores\n",
    "  else:\n",
    "    return list(df_filtered[[metric]].values.flat)\n",
    "\n",
    "def validate(annotations):\n",
    "  num_items = annotations[next(iter(annotations))].shape[0]\n",
    "  # Ensure all entries have been annotated.\n",
    "  for annotator, df_annotations in annotations.items():\n",
    "    if df_annotations.shape[0] != num_items:\n",
    "      raise Exception('Number of entries do not match.')\n",
    "    if (df_annotations['annotated'] == False).any():\n",
    "      raise Exception('Not all entries are annotated by {annotator}.')\n",
    "\n",
    "def load_annotations(annotators):\n",
    "  annotations = {}\n",
    "  for annotator in annotators:\n",
    "    file = ANNOTATION_FILE_FORMAT.format(GDRIVE_BASE, annotator)\n",
    "    df_annotations = pd.read_csv(file)\n",
    "    annotations[annotator] = df_annotations\n",
    "  validate(annotations)\n",
    "  return annotations\n",
    "\n",
    "def krippendorff_scores(annotations):\n",
    "  scores = []\n",
    "  # Do we need Krippendorff score at each metric level?\n",
    "  for metric in METRICS:\n",
    "    scores = [scores_for_krippendorff(annotator, annotations[annotator], human_top_score=False, metric=metric) for annotator in ANNOTATORS]\n",
    "    k_score = krippendorff.alpha(reliability_data=scores, level_of_measurement='ordinal')\n",
    "    scores.append(['all', True, False, 'all', f'alpha_{metric}', k_score]) #model, model_top_score, human_top_score, annotator, metric, score\n",
    "    scores = [scores_for_krippendorff(annotator, annotations[annotator], human_top_score=True, metric=metric) for annotator in ANNOTATORS]\n",
    "    k_score = krippendorff.alpha(reliability_data=scores, level_of_measurement='ordinal')\n",
    "    scores.append(['all', False, True, 'all', f'alpha_{metric}', k_score]) #model, model_top_score, human_top_score, annotator, metric, score\n",
    "\n",
    "  scores = [scores_for_krippendorff(annotator, annotations[annotator], human_top_score=False, metric=None) for annotator in ANNOTATORS]\n",
    "  k_score = krippendorff.alpha(reliability_data=scores, level_of_measurement='ordinal')\n",
    "  scores.append(['all', True, False, 'all', 'alpha_all', k_score]) #model, model_top_score, human_top_score, annotator, metric, score\n",
    "\n",
    "  scores = [scores_for_krippendorff(annotator, annotations[annotator], human_top_score=True, metric=None) for annotator in ANNOTATORS]\n",
    "  k_score = krippendorff.alpha(reliability_data=scores, level_of_measurement='ordinal')\n",
    "  scores.append(['all', False, True, 'all', 'alpha_all', k_score]) #model, model_top_score, human_top_score, annotator, metric, score\n",
    "  return scores\n",
    "\n",
    "def model_scores(annotations, human_top_score):\n",
    "  # Select the top human scored items.\n",
    "  model_top_score = !human_top_score\n",
    "  mean_scores = []\n",
    "  for model in MODELS:\n",
    "    if model == 'baseline':\n",
    "      for annotator in ANNOTATORS:\n",
    "        df = annotations[annotator]\n",
    "        df_filtered = df[((df.human_top_score==human_top_score) & (df.name==model))]\n",
    "        for metric in METRICS:\n",
    "          mean = df_filtered[metric].mean()\n",
    "          mean_scores.append([model, model_top_score, human_top_score, annotator, metric, mean])\n",
    "  # For each metric calculate the average score for each model\n",
    "  # Calculate overall average for each model.\n",
    "  return mean_scores\n",
    "\n",
    "def score_all():\n",
    "  all_scores = []\n",
    "  annotations = load_annotations(ANNOTATORS)\n",
    "  all_scores = krippendorff_scores(annotations)\n",
    "  \n",
    "  # all_scores.extend(model_scores(annotations, human_top_score=True))\n",
    "  # all_scores.extend(model_scores(annotations, human_top_score=False))\n",
    "  df_scores = pd.DataFrame(all_scores, columns=['model', 'model_top_score', 'human_top_score', 'annotator', 'metric', 'score'])\n",
    "\n",
    "  # Save the model-annotator-metric-mean data.\n",
    "  df_scores.to_csv(FINAL_SCORE_SUMMARY_FILE, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12af4wR6KwAe",
   "metadata": {
    "id": "12af4wR6KwAe"
   },
   "outputs": [],
   "source": [
    "score_all()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RIepPh55dtNF",
    "usJ89qZj6y3i"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
