{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wHUaO6-oCKak",
   "metadata": {
    "id": "wHUaO6-oCKak"
   },
   "source": [
    "# T5 Tuned - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97nY_nL9CeiJ",
   "metadata": {
    "id": "97nY_nL9CeiJ"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egOvt-Oq0M2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16049,
     "status": "ok",
     "timestamp": 1680999814284,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "egOvt-Oq0M2J",
    "outputId": "b7f6b95e-9fed-467e-83eb-089332e3d801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "GDRIVE_BASE = 'drive/MyDrive/MIDS/w266/project/'\n",
    "\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtUED7nKSww0",
   "metadata": {
    "id": "RtUED7nKSww0"
   },
   "source": [
    "## Connect to Google Drive\n",
    "We will be loading data from google drive and also save trained models to google drive. So lets mount google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DvSvLEFgSxWH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15644,
     "status": "ok",
     "timestamp": 1680999829923,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "DvSvLEFgSxWH",
    "outputId": "f4bb1b06-d475-4972-c182-db452256a322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "sys.path.insert(0, GDRIVE_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l_dKpuBSaL4z",
   "metadata": {
    "id": "l_dKpuBSaL4z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5QyOXIFTC1kO",
   "metadata": {
    "id": "5QyOXIFTC1kO"
   },
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "iyD4NJOTDDlt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1681001783226,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iyD4NJOTDDlt",
    "outputId": "de42e9de-3bdd-4c22-9de4-b8915a6f5652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "common.__version__: 1.4\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, OPTForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "from collections import deque\n",
    "import common\n",
    "\n",
    "print(f'common.__version__: {common.__version__}')\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "tuning_configs = common.create_configs(GDRIVE_BASE, None, None, None, None)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UBU7IM3uc14g",
   "metadata": {
    "id": "UBU7IM3uc14g"
   },
   "source": [
    "# Story Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2r-E3q4xCa4z",
   "metadata": {
    "id": "2r-E3q4xCa4z"
   },
   "outputs": [],
   "source": [
    "class StoryBot:\n",
    "  \"\"\"Class to mimic a bot that continues the story.\"\"\"\n",
    "  def __init__(self, inferencer, n_iters=20, lines_to_use=1):\n",
    "    \"\"\"\n",
    "      Creates the interactive story bot.\n",
    "      inferencer - class to use for generating lines of the story.\n",
    "      n_iters - number of iterations to do for story generation.\n",
    "      lines_to_use - The number of lines from the story to use as context for \n",
    "                     generating the next line.\n",
    "    \"\"\"\n",
    "    self._n_iters = n_iters\n",
    "    self.lines_to_use = lines_to_use\n",
    "    self.inferencer = inferencer\n",
    "    self.re_init()\n",
    "\n",
    "  def re_init(self):\n",
    "    self.story = []\n",
    "    # Initialize queue to hold just the last \"lines_to_use\" lines of story.\n",
    "    self.context_lines = deque([], self.lines_to_use)\n",
    "\n",
    "  def display_line_choices(self, output_lines):\n",
    "    print('Choose the line of your choice:')\n",
    "    for i, line in enumerate(output_lines):\n",
    "      print(f'{i}:', line)\n",
    "    print(f'{i+1}: Regenerate')\n",
    "    print(f'{i+2}: End')\n",
    "\n",
    "  def get_user_choice(self):\n",
    "    output_lines = self.inferencer(self.context_lines)\n",
    "    if len(output_lines) > 1:\n",
    "      self.display_line_choices(output_lines)\n",
    "      user_opt = -1\n",
    "      while user_opt == -1:\n",
    "        try:\n",
    "            user_input = input('Input the number of your choice (or ): ')\n",
    "            user_opt = int(user_input)\n",
    "            if user_opt < len(output_lines):\n",
    "              return output_lines[user_opt]\n",
    "            elif user_opt == len(output_lines):\n",
    "              return 'regenerate'\n",
    "            elif user_opt == len(output_lines) + 1:\n",
    "              return 'end'\n",
    "        except ValueError:\n",
    "            user_opt = -1\n",
    "    else:\n",
    "      return output_lines[0]\n",
    "\n",
    "  def print_story(self):\n",
    "    for i, line in enumerate(self.story):\n",
    "      if i%2 == 0:\n",
    "        print(f'User: {line}') \n",
    "      else:\n",
    "        print(f'Generated: {line}') \n",
    "\n",
    "  def __call__(self):\n",
    "    print('*'*50)\n",
    "    print('Welcome to StoryBot!\\n')\n",
    "    print('This program simulates an MMS kind of interaction with a bot to create a story sequentially.')\n",
    "    print('When the prompt appears below, start typing as if it were the input on your mobile.')\n",
    "    print('Enter end to end the story and restart to restart.') \n",
    "    print('*'*50, '\\n')\n",
    "    restart = False\n",
    "    i = 0\n",
    "    while i < self._n_iters:\n",
    "      if i > 0:\n",
    "        print('The story so far:')\n",
    "        self.print_story()\n",
    "      i = i + 1\n",
    "      # get the sentence from the user\n",
    "      sentence_in = input('Enter next line (or end): ').strip()\n",
    "      # accomodate special prompts\n",
    "      if sentence_in == 'end':\n",
    "        break\n",
    "      if sentence_in == 'restart':\n",
    "        i = 0\n",
    "        self.re_init()\n",
    "        continue\n",
    "      self.context_lines.append(sentence_in)\n",
    "      self.story.append(sentence_in)\n",
    "      output = 'regenerate'\n",
    "      while output == 'regenerate':\n",
    "        output = self.get_user_choice()\n",
    "      if output == 'end':\n",
    "        break\n",
    "      self.context_lines.append(output)\n",
    "      self.story.append(output)\n",
    "\n",
    "    print()\n",
    "    print('\\n======== Final story: =========\\n')\n",
    "    self.print_story()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2NZCLlzNxAII",
   "metadata": {
    "id": "2NZCLlzNxAII"
   },
   "outputs": [],
   "source": [
    "def run_story_bot(config, device):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "  if config.model_family == 't5':\n",
    "    model = T5ForConditionalGeneration.from_pretrained(config.tuned_model_path).to(device)\n",
    "    inferencer = common.T5Inferencer(device, model, tokenizer, prompt=config.prompt)\n",
    "  else:\n",
    "    model = OPTForCausalLM.from_pretrained(config.model_name).to(device)\n",
    "    inferencer = common.OptInferencer(device, model, tokenizer)\n",
    "  story_bot = StoryBot(inferencer, n_iters=1, lines_to_use=1)\n",
    "  story_bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BOPjAniJPRPW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36348,
     "status": "ok",
     "timestamp": 1681000226550,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "BOPjAniJPRPW",
    "outputId": "e2a3bcc9-8820-4d50-b4c3-aa8783131cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Welcome to StoryBot!\n",
      "\n",
      "This program simulates an MMS kind of interaction with a bot to create a story sequentially.\n",
      "When the prompt appears below, start typing as if it were the input on your mobile.\n",
      "Enter end to end the story and restart to restart.\n",
      "************************************************** \n",
      "\n",
      "Enter next line (or end): The little girl felt very lonely and scared.\n",
      "Choose the line of your choice:\n",
      "0: \"It's a good thing,\" said she.\n",
      "1: \"It's a very pleasant place,\" said the little girl.\n",
      "2: \"It's all right,\" she said.\n",
      "3: Regenerate\n",
      "4: End\n",
      "Input the number of your choice (or ): 2\n",
      "\n",
      "\n",
      "======== Final story: =========\n",
      "\n",
      "User: The little girl felt very lonely and scared.\n",
      "Generated: \"It's all right,\" she said.\n"
     ]
    }
   ],
   "source": [
    "# Run story bot on t5 s1\n",
    "run_story_bot(tuning_configs['t5_s1'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iJvegk-rK13d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28706,
     "status": "ok",
     "timestamp": 1681000342592,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iJvegk-rK13d",
    "outputId": "ec8b13f3-6ec6-48a1-ebc3-3b925111c2e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Welcome to StoryBot!\n",
      "\n",
      "This program simulates an MMS kind of interaction with a bot to create a story sequentially.\n",
      "When the prompt appears below, start typing as if it were the input on your mobile.\n",
      "Enter end to end the story and restart to restart.\n",
      "************************************************** \n",
      "\n",
      "Enter next line (or end): The little girl felt very lonely and scared.\n",
      "Choose the line of your choice:\n",
      "0: The little girl felt very lonely and scared. She was crying and crying, and her parents were crying with her, but she didn't want to talk to them because she couldn't say anything, so she just sat there and cried for a\n",
      "1: The little girl felt very lonely and scared. She wanted to go to school, but she didn't know where to find a teacher to help her. The teacher told her that she had to get a diploma to be accepted into a school.\n",
      "2: The little girl felt very lonely and scared.  She was called to the hospital to be checked out, but she was not feeling well. She was scared and didn't know what to do next. After that, she went home and slept\n",
      "3: Regenerate\n",
      "4: End\n",
      "Input the number of your choice (or ): 2\n",
      "\n",
      "\n",
      "======== Final story: =========\n",
      "\n",
      "User: The little girl felt very lonely and scared.\n",
      "Generated: The little girl felt very lonely and scared.  She was called to the hospital to be checked out, but she was not feeling well. She was scared and didn't know what to do next. After that, she went home and slept\n"
     ]
    }
   ],
   "source": [
    "# Run story bot on opt s2\n",
    "run_story_bot(tuning_configs['opt_s2'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6TUt1hxpKvDW",
   "metadata": {
    "id": "6TUt1hxpKvDW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "RIepPh55dtNF",
   "metadata": {
    "id": "RIepPh55dtNF"
   },
   "source": [
    "# Batch Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "B3-6aY66dvO_",
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1681006593094,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "B3-6aY66dvO_"
   },
   "outputs": [],
   "source": [
    "def generate_next_line(config, lines, device, num_sequences=5):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "  for i, test_input_text in enumerate(lines):\n",
    "      test_inputs = tokenizer([config.prompt + test_input_text], return_tensors='pt')\n",
    "      input_ids = test_inputs['input_ids'].to(device)\n",
    "      if config.model_family == 't5':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(config.tuned_model_path).to(device)\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            num_beams=num_sequences,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_return_sequences=num_sequences,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            renormalize_logits=True,\n",
    "            # normalize_logits=True,\n",
    "          )\n",
    "      else:\n",
    "        model = OPTForCausalLM.from_pretrained(config.model_name).to(device)  \n",
    "        outputs = model.generate(\n",
    "          input_ids,\n",
    "          num_beams=num_sequences,\n",
    "          no_repeat_ngram_size=2,\n",
    "          num_return_sequences=num_sequences,\n",
    "          max_length = 50,\n",
    "          do_sample=True,\n",
    "          top_k=0,\n",
    "          early_stopping=True,\n",
    "          return_dict_in_generate=True,\n",
    "          output_scores=True,\n",
    "          renormalize_logits=True\n",
    "        )\n",
    "\n",
    "      scores = -1 * outputs.sequences_scores.cpu()\n",
    "      test_output_ids = outputs.sequences\n",
    "      decoded = [tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).replace('\\n', ' ') for out_ids in test_output_ids]\n",
    "      print(f'Input: {test_input_text}')\n",
    "      # decoded = '\\n\\t'.join(decoded)\n",
    "      # print(f'Output: {decoded}')\n",
    "      for (score, output) in zip(scores, decoded):\n",
    "        print(f'\\t{score}: {output}')\n",
    "\n",
    "def generate_for(config, device, first_lines):\n",
    "  print('*' *50)\n",
    "  print(f'Evaluating {config.model_name} tuned on {config.dataset} dataset')\n",
    "  generate_next_line(config, first_lines, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "z6pXWfANgeE3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22525,
     "status": "ok",
     "timestamp": 1681006616234,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "z6pXWfANgeE3",
    "outputId": "891400b3-8030-4590-9e43-58a051fc137b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Evaluating google/t5-v1_1-base tuned on s1 dataset\n",
      "Input: Lara felt very sad and scared.\n",
      "\t3.085955904680304e-05: “I don't know,” said her mother.\n",
      "\t3.301061951788142e-05: \"I'll be able to change a few things,\" he said.\n",
      "\t0.00010224935977021232: \"My dear, I'm afraid that you are too fond of me, and you must be afraid.\n",
      "\t3.072101026191376e-05: \"It's time for a change,\" she said, looking up.\n",
      "\t2.518398832762614e-05: Then, seated herself upon the bed, and began to scream, “Then I'll go and get hold of my sword.”\n",
      "Input: All the dragons of the world lived on one mountain called the dragon mountain.\n",
      "\t8.454718044959009e-05: The dragon was snatched up by a peasant who was run off as swiftly as a wolf.\n",
      "\t1.8762297258945182e-05: But the dragon was so strong that the dragons were terribly frightened and they called to the King and said, “Now you go down here, and I will tell you what I have done.”\n",
      "\t8.076785888988525e-05: When they reached the cave, the dragon was a great giant and was buried in the earth, but the dragons had buried them all.\n",
      "\t0.00011580472346395254: It was a place of great danger to the dragon, and the dragon could not get out of the mountain.\n",
      "\t5.294632137520239e-05: The dragons came and said: “Thou art a dragon, the dragon is the dragon of the sea.”\n",
      "**************************************************\n",
      "Evaluating facebook/opt-350m tuned on s2 dataset\n",
      "Input: Lara felt very sad and scared.\n",
      "\t0.0054016392678022385: Lara felt very sad and scared. She tried to comfort herself by telling herself to go back to the house and keep her head down. However, her body was so heavy that she couldn’t move. The last thing she remembered was\n",
      "\t0.011608454398810863: Lara felt very sad and scared.  She said, \"You don't understand. You don’t understand what is going on. I can‘t even tell you what I am going through because I cannot imagine it.\n",
      "\t0.014569845050573349: Lara felt very sad and scared. She was not sure if there was any way to help her. She was afraid she would be alone forever and never be able to come back. And she did not want to live in a world where\n",
      "\t0.01579592004418373: Lara felt very sad and scared. She had never seen her father for the first time in a long time, but she wanted to make him proud and wanted everyone to be proud of him, too. But she was afraid that she would\n",
      "\t0.010952787473797798: Lara felt very sad and scared. She started to cry. \"What am I doing here?\" she asked, still holding her hand in front of her face. She turned around and saw a small group of soldiers standing around the table.\n",
      "Input: All the dragons of the world lived on one mountain called the dragon mountain.\n",
      "\t0.020541148260235786: All the dragons of the world lived on one mountain called the dragon mountain. They each were different shades of grey and had their own personality. I like the way you think.\n",
      "\t0.0021268322598189116: All the dragons of the world lived on one mountain called the dragon mountain. But they didn't live long enough.  Their life spans would have been short in comparison.\n",
      "\t0.009853065945208073: All the dragons of the world lived on one mountain called the dragon mountain. This is a correct answer! Thank you!\n",
      "\t0.02022973634302616: All the dragons of the world lived on one mountain called the dragon mountain. I think I know what you mean, but I've only ever heard that when you say \"dragon mountain\" Yeah I can see it too.\n",
      "\t0.021370140835642815: All the dragons of the world lived on one mountain called the dragon mountain. And their house was built by dragons to live on the same mountain as dragons.\n"
     ]
    }
   ],
   "source": [
    "first_lines = ['Lara felt very sad and scared.', 'All the dragons of the world lived on one mountain called the dragon mountain.']\n",
    "\n",
    "generate_for(tuning_configs['t5_s1'], device, first_lines)\n",
    "generate_for(tuning_configs['opt_s2'], device, first_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uHDGp3AUhKkP",
   "metadata": {
    "id": "uHDGp3AUhKkP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UBU7IM3uc14g"
   ],
   "provenance": [
    {
     "file_id": "1qH9or9UvtJOb_mlSSHcRjZKwZT9uDwbl",
     "timestamp": 1680142704332
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
