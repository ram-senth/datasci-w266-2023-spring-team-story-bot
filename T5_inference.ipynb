{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wHUaO6-oCKak",
   "metadata": {
    "id": "wHUaO6-oCKak"
   },
   "source": [
    "# T5 Tuned - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97nY_nL9CeiJ",
   "metadata": {
    "id": "97nY_nL9CeiJ"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "egOvt-Oq0M2J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22948,
     "status": "ok",
     "timestamp": 1680361541662,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "egOvt-Oq0M2J",
    "outputId": "81790bac-7439-4473-e78b-ebd40d6f806f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtUED7nKSww0",
   "metadata": {
    "id": "RtUED7nKSww0"
   },
   "source": [
    "## Connect to Google Drive\n",
    "We will be loading data from google drive and also save trained models to google drive. So lets mount google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "DvSvLEFgSxWH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27848,
     "status": "ok",
     "timestamp": 1680361569506,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "DvSvLEFgSxWH",
    "outputId": "bc579be2-ad00-465b-ed15-0914e932600d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5QyOXIFTC1kO",
   "metadata": {
    "id": "5QyOXIFTC1kO"
   },
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iyD4NJOTDDlt",
   "metadata": {
    "executionInfo": {
     "elapsed": 7081,
     "status": "ok",
     "timestamp": 1680361576584,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iyD4NJOTDDlt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from collections import deque\n",
    "\n",
    "DATA_NAME = \"s2\"\n",
    "# T5_MODEL_NAME = \"t5-small\"\n",
    "T5_MODEL_NAME = \"t5-base\"\n",
    "\n",
    "TUNED_T5_SAVED = f'drive/MyDrive/MIDS/w266/project/saved_models/{T5_MODEL_NAME}-data{DATA_NAME}-finetuned'\n",
    "PROMPT = 'Generate next line: '\n",
    "\n",
    "FINAL_TEST_LIST = ['Princess Leia lay upon her bed all the night.',\n",
    "                   'He stopped himself for a minute and thought if it was the right thing to do.',\n",
    "                   'There once lived king named Rama.',\n",
    "                   'Once upon a time, an old Owl lived in the forest.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fw6I0qfQ0U5z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "715a9ba3518f4a5ab5392769f5cbe5e5",
      "ed8fa87a298647809575942feccb32e3",
      "82c5acf54d8742f486dc8a5e5a8324e9",
      "4f7ad42d69ce4e5fbd9852216dcbe581",
      "45ca77e2ab734378bfc239cd90c58d1a",
      "6248dc0ddc9f47f68f5a27578e5dd737",
      "3025f526b52e47bcba769c0bc1b83990",
      "22ae419c24094857abc86144bea3e5f4",
      "c0391b9a32974c9e9df7c7a96aa59044",
      "20511b48951b4b129fa735f7ca645a90",
      "0e11a9f4b9bc4958ba18b313039e8449",
      "fd77b4d1d5e34ad88b5e7593624472e9",
      "f678a4ce00bc4f04b31fa21d2423264c",
      "d0c5cf0169c04c20914c61fb53509028",
      "1f7d724dc7ca45f3b1cd7bc5029feb31",
      "496fc30ecc1045cea394e075d81eb8cc",
      "ea9ba36a4cb7465196f997631481c628",
      "54204e52f09c4c7fb8ea451f55942477",
      "b7bd7afccc5d47b480e0f07e945fd081",
      "f15ad4a614694be697c68939c5ab53c9",
      "9c4b8231da604d4d937f03d937651c0e",
      "ca8728926c8e4b7f904bdae84a4c76ab"
     ]
    },
    "executionInfo": {
     "elapsed": 6832,
     "status": "ok",
     "timestamp": 1680361583413,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "fw6I0qfQ0U5z",
    "outputId": "1fa19d7f-da4a-423d-c0b5-b198384117aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715a9ba3518f4a5ab5392769f5cbe5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd77b4d1d5e34ad88b5e7593624472e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UBU7IM3uc14g",
   "metadata": {
    "id": "UBU7IM3uc14g"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9WyuXHzJUTSY",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1680361583413,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "9WyuXHzJUTSY"
   },
   "outputs": [],
   "source": [
    "class Inferencer:\n",
    "  def __init__(self, model, tokenizer, prompt='', max_new_tokens=100, tensor_type='pt', num_beams=3):\n",
    "    self.model = model\n",
    "    self.tokenizer = tokenizer\n",
    "    self.prompt = prompt\n",
    "    self.max_new_tokens = max_new_tokens\n",
    "    self.tensor_type = tensor_type\n",
    "    self.num_beams = num_beams\n",
    "\n",
    "  def __call__(self, context_lines):\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    test_inputs = self.tokenizer([self.prompt + ' '.join(context_lines)], return_tensors=self.tensor_type)\n",
    "    test_output_ids = model.generate(\n",
    "        test_inputs['input_ids'].cuda(),\n",
    "        num_beams=self.num_beams,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=self.num_beams,\n",
    "        max_new_tokens=self.max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=0)\n",
    "    decoded = [self.tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for out_ids in test_output_ids]\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2r-E3q4xCa4z",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680361604036,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "2r-E3q4xCa4z"
   },
   "outputs": [],
   "source": [
    "class StoryBot:\n",
    "  \"\"\"Class to mimic a bot that continues the story.\"\"\"\n",
    "  def __init__(self, inferencer, n_iters=20, lines_to_use=1):\n",
    "    \"\"\"\n",
    "      Creates the interactive story bot.\n",
    "      inferencer - class to use for generating lines of the story.\n",
    "      n_iters - number of iterations to do for story generation.\n",
    "      lines_to_use - The number of lines from the story to use as context for \n",
    "                     generating the next line.\n",
    "    \"\"\"\n",
    "    self._n_iters = n_iters\n",
    "    self.lines_to_use = lines_to_use\n",
    "    self.inferencer = inferencer\n",
    "    self.re_init()\n",
    "\n",
    "  def re_init(self):\n",
    "    self.story = []\n",
    "    # Initialize queue to hold just the last \"lines_to_use\" lines of story.\n",
    "    self.context_lines = deque([], self.lines_to_use)\n",
    "\n",
    "  def display_line_choices(self, output_lines):\n",
    "    print('Choose the line of your choice:')\n",
    "    for i, line in enumerate(output_lines):\n",
    "      print(f'{i}:', line)\n",
    "    print(f'{i+1}: Regenerate')\n",
    "    print(f'{i+2}: End')\n",
    "\n",
    "  def get_user_choice(self):\n",
    "    output_lines = self.inferencer(self.context_lines)\n",
    "    if len(output_lines) > 1:\n",
    "      self.display_line_choices(output_lines)\n",
    "      user_opt = -1\n",
    "      while user_opt == -1:\n",
    "        try:\n",
    "            user_input = input('Input the number of your choice (or ): ')\n",
    "            user_opt = int(user_input)\n",
    "            if user_opt < len(output_lines):\n",
    "              return output_lines[user_opt]\n",
    "            elif user_opt == len(output_lines):\n",
    "              return 'regenerate'\n",
    "            elif user_opt == len(output_lines) + 1:\n",
    "              return 'end'\n",
    "        except ValueError:\n",
    "            user_opt = -1\n",
    "    else:\n",
    "      return output_lines[0]\n",
    "\n",
    "  def print_story(self):\n",
    "    for i, line in enumerate(self.story):\n",
    "      if i%2 == 0:\n",
    "        print(f'User: {line}') \n",
    "      else:\n",
    "        print(f'Generated: {line}') \n",
    "\n",
    "  def __call__(self):\n",
    "    print('*'*50)\n",
    "    print('Welcome to StoryBot!\\n')\n",
    "    print('This program simulates an MMS kind of interaction with a bot to create a story sequentially.')\n",
    "    print('When the prompt appears below, start typing as if it were the input on your mobile.')\n",
    "    print('Enter end to end the story and restart to restart.') \n",
    "    print('*'*50, '\\n')\n",
    "    restart = False\n",
    "    i = 0\n",
    "    while i < self._n_iters:\n",
    "      if i > 0:\n",
    "        print('The story so far:')\n",
    "        self.print_story()\n",
    "      i = i + 1\n",
    "      # get the sentence from the user\n",
    "      sentence_in = input('Enter next line (or end): ').strip()\n",
    "      # accomodate special prompts\n",
    "      if sentence_in == 'end':\n",
    "        break\n",
    "      if sentence_in == 'restart':\n",
    "        i = 0\n",
    "        self.re_init()\n",
    "        continue\n",
    "      self.context_lines.append(sentence_in)\n",
    "      self.story.append(sentence_in)\n",
    "      output = 'regenerate'\n",
    "      while output == 'regenerate':\n",
    "        output = self.get_user_choice()\n",
    "      if output == 'end':\n",
    "        break\n",
    "      self.context_lines.append(output)\n",
    "      self.story.append(output)\n",
    "\n",
    "    print()\n",
    "    print('\\n======== Final story: =========\\n')\n",
    "    self.print_story()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2NZCLlzNxAII",
   "metadata": {
    "executionInfo": {
     "elapsed": 31679,
     "status": "ok",
     "timestamp": 1680361639756,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "2NZCLlzNxAII"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(TUNED_T5_SAVED).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "BOPjAniJPRPW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133923,
     "status": "ok",
     "timestamp": 1680361782371,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "BOPjAniJPRPW",
    "outputId": "685aff23-a11f-4fc1-dded-56b5a882a0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Welcome to StoryBot!\n",
      "\n",
      "This program simulates an MMS kind of interaction with a bot to create a story sequentially.\n",
      "When the prompt appears below, start typing as if it were the input on your mobile.\n",
      "Enter end to end the story and restart to restart.\n",
      "************************************************** \n",
      "\n",
      "Enter next line (or end): Once upon a time, an old Owl lived in the forest.\n",
      "Choose the line of your choice:\n",
      "0: The Owl, however, was not a wild animal, and he did not like to be disturbed by the twilight of the night.\n",
      "1: He had many a day's work to do, but he would not let it go.\n",
      "2: He was a solitary creature, and he lived in the forest.\n",
      "3: Regenerate\n",
      "4: End\n",
      "Input the number of your choice (or ): 2\n",
      "The story so far:\n",
      "User: Once upon a time, an old Owl lived in the forest.\n",
      "Generated: He was a solitary creature, and he lived in the forest.\n",
      "Enter next line (or end): The owl did not like talking to any of the other animals.\n",
      "Choose the line of your choice:\n",
      "0: He was so afraid that he could not go anywhere.\n",
      "1: He was a solitary creature, and he did not like to talk to any of the other animals.\n",
      "2: He did not like to be talked to.\n",
      "3: Regenerate\n",
      "4: End\n",
      "Input the number of your choice (or ): 0\n",
      "The story so far:\n",
      "User: Once upon a time, an old Owl lived in the forest.\n",
      "Generated: He was a solitary creature, and he lived in the forest.\n",
      "User: The owl did not like talking to any of the other animals.\n",
      "Generated: He was so afraid that he could not go anywhere.\n",
      "Enter next line (or end): end\n",
      "\n",
      "\n",
      "======== Final story: =========\n",
      "\n",
      "User: Once upon a time, an old Owl lived in the forest.\n",
      "Generated: He was a solitary creature, and he lived in the forest.\n",
      "User: The owl did not like talking to any of the other animals.\n",
      "Generated: He was so afraid that he could not go anywhere.\n"
     ]
    }
   ],
   "source": [
    "inferencer = Inferencer(model, tokenizer, prompt=PROMPT)\n",
    "story_bot = StoryBot(inferencer, n_iters=10, lines_to_use=5)\n",
    "story_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fV2GYVEqIrQ8",
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1680310345496,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "fV2GYVEqIrQ8"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, lines, prompt, contrastive=True):\n",
    "  transformers.logging.set_verbosity_error()\n",
    "  for test_input_text in lines:\n",
    "      test_inputs = tokenizer([prompt + test_input_text], return_tensors='pt')\n",
    "      if contrastive:\n",
    "        test_output_ids = model.generate(\n",
    "            test_inputs['input_ids'].cuda(), \n",
    "            penalty_alpha=0.4, top_k=5, max_length=256)\n",
    "      else:\n",
    "        test_output_ids = model.generate(\n",
    "            test_inputs['input_ids'].cuda(),\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            num_return_sequences=5,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_k=0)\n",
    "      print(f'Input: {test_input_text}')\n",
    "      decoded = [tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for out_ids in test_output_ids]\n",
    "      print(f'Output: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "iJvegk-rK13d",
   "metadata": {
    "executionInfo": {
     "elapsed": 3852,
     "status": "ok",
     "timestamp": 1680308928818,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "iJvegk-rK13d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jFD-hJKQTnHd",
   "metadata": {
    "id": "jFD-hJKQTnHd"
   },
   "outputs": [],
   "source": [
    "def infer_nextline(model, tokenizer, lines, prompt, contrastive=True):\n",
    "  transformers.logging.set_verbosity_error()\n",
    "  for test_input_text in lines:\n",
    "      test_inputs = tokenizer([prompt + test_input_text], return_tensors='pt')\n",
    "      if contrastive:\n",
    "        test_output_ids = model.generate(\n",
    "            test_inputs['input_ids'].cuda(), \n",
    "            penalty_alpha=0.4, top_k=5, max_length=256)\n",
    "      else:\n",
    "        test_output_ids = model.generate(\n",
    "            test_inputs['input_ids'].cuda(),\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            num_return_sequences=5,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_k=0)\n",
    "      print(f'Input: {test_input_text}')\n",
    "      decoded = [tokenizer.decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for out_ids in test_output_ids]\n",
    "      print(f'Output: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wxim_UyuNkF1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48960,
     "status": "ok",
     "timestamp": 1680224027940,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "wxim_UyuNkF1",
    "outputId": "63e705c4-e5ac-4ff2-def4-47b665f813d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Contrastive Search ====\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['She slept in a slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering, slumbering']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['“It is a good thing to do,” said he, “and I am sure you will be able to do it.”']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['He was a great king, and he was the most powerful.']\n",
      "Input: Once upon a time, an old Owl lived in the forest.\n",
      "Output: ['He sat on a tree, and he sat down to eat the berries.']\n",
      "==== Beam Search ====\n",
      "Input: Princess Leia lay upon her bed all the night.\n",
      "Output: ['She was so tired that she could not sleep.', 'She was so dreadful that she felt as if she had lost her way, and she did not know what to do.', 'Then she went to sleep, and when she had woken up, she sat down in the corner of the bed.', 'Then she sat down and cried, “It’s a good night, Princess Leia!”', 'Then she woke up in the middle of the night.']\n",
      "Input: He stopped himself for a minute and thought if it was the right thing to do.\n",
      "Output: ['\"It\\'s the right thing to do,\" he said.', '“It is a good thing to do,” he said.', 'And he said, “It is a good thing to do.', '“It is a great deal of work to do,” said he.', '“Well, if it was a right thing to do,” he said.']\n",
      "Input: There once lived king named Rama.\n",
      "Output: ['He was a king of the gods.', 'Rama was a very good man, and he was very wise.', 'Rama was a king of the desert.', 'Rama was a great king, and he ruled over all the world.', 'He was a king, and he commanded all his people to obey him.']\n",
      "Input: Once upon a time, an old Owl lived in the forest.\n",
      "Output: ['He had a great deal of money, and he was very good to him.', 'He was a frog, and he lived in an old hut.', 'He was very poor, but he was not a good one, and did not like to be scolded.', 'He was a good-natured creature.', 'He had a great deal of money, and he had lots of time to spare.']\n"
     ]
    }
   ],
   "source": [
    "## Fine tuned T5 model\n",
    "print(\"==== Contrastive Search ====\")\n",
    "evaluate(model, t5_tokenizer, FINAL_TEST_LIST, PROMPT, contrastive=True)\n",
    "\n",
    "print(\"==== Beam Search ====\")\n",
    "evaluate(model, t5_tokenizer, FINAL_TEST_LIST, PROMPT, contrastive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6TUt1hxpKvDW",
   "metadata": {
    "id": "6TUt1hxpKvDW"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "kFX3MIARK7hh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4241,
     "status": "ok",
     "timestamp": 1680310367160,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "kFX3MIARK7hh",
    "outputId": "d16d6d28-a900-4126-82fd-e51e84a6b7ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A rich old farmer, who felt that he had not many more days to live, called his sons to his bedside.\n",
      "Output: ['They sat down on a bed, and looked at each other.', 'They said to him, “It is a good thing that thou shalt not die.”', '\"I am a farmer,\" he said, \"and I do not know what to do with my sons.\"', 'The sons of the poor farmer were very happy, but they did not want to see him, and so he gave them a supper.', 'The sons sat in the bed, and cried out: \"Let us go to bed.\"']\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, t5_tokenizer, ['A rich old farmer, who felt that he had not many more days to live, called his sons to his bedside.'], PROMPT, contrastive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2obsxS6BTQtR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3102,
     "status": "ok",
     "timestamp": 1680226510883,
     "user": {
      "displayName": "Ram S",
      "userId": "17279396566363655986"
     },
     "user_tz": 420
    },
    "id": "2obsxS6BTQtR",
    "outputId": "4f5387c9-444b-494b-ad7a-a4afb383fa59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Then he sat down and talked to his sons.\n",
      "Output: ['\"I don\\'t know,\" said he.', 'Then he sat down and talked to them.', 'Then he said to his sons, “I am very sorry for you, but I don’t know what to do with you.”', '\"It\\'s a good thing,\" he said.', '\"What do you think?\" asked he.']\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, t5_tokenizer, ['Then he sat down and talked to his sons.'], PROMPT, contrastive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqMMiBfCCla_",
   "metadata": {
    "id": "iqMMiBfCCla_"
   },
   "outputs": [],
   "source": [
    "story_bot = GPTStoryBot()\n",
    "story_bot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zLsxWvfmP-IW",
   "metadata": {
    "id": "zLsxWvfmP-IW"
   },
   "source": [
    "## Round 1\n",
    "\n",
    "**Prompt**: Once upon a time, an old Owl lived in the forest.\n",
    "\n",
    "**Response**: He was a good-natured creature.\n",
    "\n",
    "**Prompt**: The owl helped all animals that came to him.\n",
    "\n",
    "**Response**: The owl was a very good man, and he was very kind to the animals that came to him.\n",
    "\n",
    "**Prompt**: One day a little mouse came to the owl.\n",
    "\n",
    "**Response**: The little mouse sat down and looked at the owl.\n",
    "\n",
    "**Prompt**: The owl saw that the mouse was very sad.\n",
    "\n",
    "**Response**: The owl said, “It is a very sad mouse.\"\n",
    "\n",
    "**Prompt**: The owl asked \"why are you so sad, mouse?\"\n",
    "\n",
    "**Response**: \"It\\'s a misfortune,\" answered the mouse.\n",
    "\n",
    "## Round 2\n",
    "**Prompt**: A rich old farmer, who felt that he had not many more days to live, called his sons to his bedside.\n",
    "**Response**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vsV697tcRxsM",
   "metadata": {
    "id": "vsV697tcRxsM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1qH9or9UvtJOb_mlSSHcRjZKwZT9uDwbl",
     "timestamp": 1680142704332
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
